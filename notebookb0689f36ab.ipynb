{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9624044,"sourceType":"datasetVersion","datasetId":5874439}],"dockerImageVersionId":31040,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# This code will prepare the dataset using multiple embedding and model techniques:\n# - TF-IDF + Dense\n# - Word2Vec + LSTM\n# - FastText + CNN\n# - BERT embeddings + Transformer layers\n\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score\nfrom gensim.models import Word2Vec, FastText\nfrom transformers import BertTokenizer, BertModel\nimport nltk\nfrom nltk.tokenize import word_tokenize\n# nltk.download(\"punkt\")\n\n# Load data\ndf = pd.read_csv(\"/kaggle/input/truth-seeker-dataset-2023-truthseeker2023/TruthSeeker2023/Truth_Seeker_Model_Dataset.csv\")\n\n# Drop NA values\ndf.dropna(subset=[\"statement\", \"tweet\", \"BinaryNumTarget\"], inplace=True)\n\n# Combine relevant text fields\ndf[\"text\"] = df[\"statement\"] + \" \" + df[\"tweet\"]\ny = df[\"BinaryNumTarget\"].astype(int).values\n\n# ----------------------------- TF-IDF + DENSE NN -----------------------------\ntfidf = TfidfVectorizer(max_features=10000)\nX_tfidf = tfidf.fit_transform(df[\"text\"])\n\nX_tfidf_train, X_tfidf_test, y_tfidf_train, y_tfidf_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n\n# Convert to dense\nX_tfidf_train_dense = X_tfidf_train.toarray()\nX_tfidf_test_dense = X_tfidf_test.toarray()\n\nclass TfidfDataset(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.long)\n    def __len__(self):\n        return len(self.y)\n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx]\n\nclass TfidfNN(nn.Module):\n    def __init__(self, input_dim):\n        super().__init__()\n        self.model = nn.Sequential(\n            nn.Linear(input_dim, 128),\n            nn.ReLU(),\n            nn.Linear(128, 2)\n        )\n    def forward(self, x):\n        return self.model(x)\n\n# Prepare TF-IDF DataLoader\ntrain_tfidf_ds = TfidfDataset(X_tfidf_train_dense, y_tfidf_train)\ntest_tfidf_ds = TfidfDataset(X_tfidf_test_dense, y_tfidf_test)\ntrain_tfidf_loader = DataLoader(train_tfidf_ds, batch_size=64, shuffle=True)\ntest_tfidf_loader = DataLoader(test_tfidf_ds, batch_size=64)\n\n# Define Word2Vec + LSTM and FastText + CNN after this\n# Generate word tokens for embedding models\ndf[\"tokens\"] = df[\"text\"].apply(lambda x: word_tokenize(x.lower()))\n\n# Train Word2Vec and FastText models\nw2v_model = Word2Vec(sentences=df[\"tokens\"], vector_size=100, window=5, min_count=2, workers=4)\nft_model = FastText(sentences=df[\"tokens\"], vector_size=100, window=5, min_count=2, workers=4)\n\n# Embed sentences using Word2Vec\ndef embed_sent_w2v(tokens):\n    vectors = [w2v_model.wv[word] for word in tokens if word in w2v_model.wv]\n    return np.mean(vectors, axis=0) if vectors else np.zeros(100)\n\nX_w2v = np.array([embed_sent_w2v(toks) for toks in df[\"tokens\"]])\nX_w2v_train, X_w2v_test, y_w2v_train, y_w2v_test = train_test_split(X_w2v, y, test_size=0.2, random_state=42)\n\n# Embed sentences using FastText\ndef embed_sent_ft(tokens):\n    vectors = [ft_model.wv[word] for word in tokens if word in ft_model.wv]\n    return np.mean(vectors, axis=0) if vectors else np.zeros(100)\n\nX_ft = np.array([embed_sent_ft(toks) for toks in df[\"tokens\"]])\nX_ft_train, X_ft_test, y_ft_train, y_ft_test = train_test_split(X_ft, y, test_size=0.2, random_state=42)\n\n# Now continue with model definitions for:\n# - LSTM for Word2Vec embeddings\n# - CNN for FastText embeddings\n# - Transformer/BERT-based embeddings next\n\n{\n    \"X_tfidf_shape\": X_tfidf.shape,\n    \"X_w2v_shape\": X_w2v.shape,\n    \"X_ft_shape\": X_ft.shape,\n    \"sample_text\": df[\"text\"].iloc[0]\n}\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T10:13:32.663758Z","iopub.execute_input":"2025-06-23T10:13:32.664088Z","iopub.status.idle":"2025-06-23T10:17:58.354953Z","shell.execute_reply.started":"2025-06-23T10:13:32.664062Z","shell.execute_reply":"2025-06-23T10:17:58.354217Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"{'X_tfidf_shape': (134198, 10000),\n 'X_w2v_shape': (134198, 100),\n 'X_ft_shape': (134198, 100),\n 'sample_text': 'End of eviction moratorium means millions of Americans could lose their housing in the middle of a pandemic. @POTUS Biden Blunders - 6 Month Update\\n\\nInflation, Delta mismanagement, COVID for kids, Abandoning Americans in Afghanistan, Arming the Taliban, S. Border crisis, Breaking job growth, Abuse of power (Many Exec Orders, $3.5T through Reconciliation, Eviction Moratorium)...what did I miss?'}"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"df['BinaryNumTarget'].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T10:37:31.591798Z","iopub.execute_input":"2025-06-23T10:37:31.592114Z","iopub.status.idle":"2025-06-23T10:37:31.605274Z","shell.execute_reply.started":"2025-06-23T10:37:31.592091Z","shell.execute_reply":"2025-06-23T10:37:31.604315Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"BinaryNumTarget\n1.0    68930\n0.0    65268\nName: count, dtype: int64"},"metadata":{}}],"execution_count":16},{"cell_type":"markdown","source":"# Word2vec + LSTM","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import TensorDataset\n\n\nclass LSTMW2V(nn.Module):\n    def __init__(self, input_dim=100, hidden_dim=64):\n        super().__init__()\n        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n        self.fc = nn.Linear(hidden_dim, 2)\n    \n    def forward(self, x):\n        _, (h_n, _) = self.lstm(x)\n        return self.fc(h_n[-1])\n\n# Convert X_w2v to 3D shape: (samples, sequence_len=1, embedding_dim)\nX_w2v_train_3d = torch.tensor(X_w2v_train[:, np.newaxis, :], dtype=torch.float32)\nX_w2v_test_3d = torch.tensor(X_w2v_test[:, np.newaxis, :], dtype=torch.float32)\ny_w2v_train = torch.tensor(y_w2v_train, dtype=torch.long)\ny_w2v_test = torch.tensor(y_w2v_test, dtype=torch.long)\n\ntrain_w2v_ds = TensorDataset(X_w2v_train_3d, y_w2v_train)\ntest_w2v_ds = TensorDataset(X_w2v_test_3d, y_w2v_test)\n\ntrain_w2v_loader = DataLoader(train_w2v_ds, batch_size=64, shuffle=True)\ntest_w2v_loader = DataLoader(test_w2v_ds, batch_size=64)\n\nmodel_w2v = LSTMW2V()\noptimizer = torch.optim.Adam(model_w2v.parameters(), lr=1e-3)\ncriterion = nn.CrossEntropyLoss()\n\n# Train loop\nfor epoch in range(50):\n    model_w2v.train()\n    total_loss = 0\n    for xb, yb in train_w2v_loader:\n        optimizer.zero_grad()\n        pred = model_w2v(xb)\n        loss = criterion(pred, yb)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T10:22:41.624898Z","iopub.execute_input":"2025-06-23T10:22:41.625421Z","iopub.status.idle":"2025-06-23T10:26:41.343628Z","shell.execute_reply.started":"2025-06-23T10:22:41.625396Z","shell.execute_reply":"2025-06-23T10:26:41.342763Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_35/2305316823.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  y_w2v_train = torch.tensor(y_w2v_train, dtype=torch.long)\n/tmp/ipykernel_35/2305316823.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  y_w2v_test = torch.tensor(y_w2v_test, dtype=torch.long)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Loss: 336.8802\nEpoch 2, Loss: 149.4038\nEpoch 3, Loss: 103.3492\nEpoch 4, Loss: 79.6817\nEpoch 5, Loss: 64.7250\nEpoch 6, Loss: 55.7825\nEpoch 7, Loss: 46.5139\nEpoch 8, Loss: 41.5372\nEpoch 9, Loss: 36.2298\nEpoch 10, Loss: 31.8420\nEpoch 11, Loss: 28.9013\nEpoch 12, Loss: 26.0275\nEpoch 13, Loss: 22.8323\nEpoch 14, Loss: 20.5549\nEpoch 15, Loss: 18.8809\nEpoch 16, Loss: 16.9253\nEpoch 17, Loss: 14.8794\nEpoch 18, Loss: 13.9634\nEpoch 19, Loss: 12.8536\nEpoch 20, Loss: 11.4497\nEpoch 21, Loss: 10.2339\nEpoch 22, Loss: 10.3583\nEpoch 23, Loss: 8.8308\nEpoch 24, Loss: 7.7048\nEpoch 25, Loss: 6.7385\nEpoch 26, Loss: 7.3439\nEpoch 27, Loss: 6.0944\nEpoch 28, Loss: 5.7073\nEpoch 29, Loss: 5.6105\nEpoch 30, Loss: 5.0560\nEpoch 31, Loss: 4.4141\nEpoch 32, Loss: 4.4664\nEpoch 33, Loss: 3.5708\nEpoch 34, Loss: 3.8614\nEpoch 35, Loss: 3.9508\nEpoch 36, Loss: 2.4993\nEpoch 37, Loss: 4.6406\nEpoch 38, Loss: 2.2884\nEpoch 39, Loss: 2.9537\nEpoch 40, Loss: 2.7268\nEpoch 41, Loss: 2.5273\nEpoch 42, Loss: 2.8547\nEpoch 43, Loss: 2.9837\nEpoch 44, Loss: 1.8285\nEpoch 45, Loss: 3.0451\nEpoch 46, Loss: 2.0606\nEpoch 47, Loss: 1.9561\nEpoch 48, Loss: 3.1391\nEpoch 49, Loss: 1.6097\nEpoch 50, Loss: 2.9776\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"def evaluate_model(model, dataloader):\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for xb, yb in dataloader:\n            outputs = model(xb)\n            _, predicted = torch.max(outputs, 1)\n            correct += (predicted == yb).sum().item()\n            total += yb.size(0)\n    return correct / total\n\naccuracy = evaluate_model(model_w2v, test_w2v_loader)\nprint(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T10:26:44.640856Z","iopub.execute_input":"2025-06-23T10:26:44.641174Z","iopub.status.idle":"2025-06-23T10:26:45.123647Z","shell.execute_reply.started":"2025-06-23T10:26:44.641148Z","shell.execute_reply":"2025-06-23T10:26:45.122845Z"}},"outputs":[{"name":"stdout","text":"Test Accuracy: 99.41%\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# FastText + CNN","metadata":{}},{"cell_type":"code","source":"class CNNFT(nn.Module):\n    def __init__(self, input_dim=100):\n        super().__init__()\n        self.conv1 = nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3)\n        self.pool = nn.MaxPool1d(2)\n        self.fc = nn.Linear(16 * 49, 2)  # Adjust based on conv+pool output size\n    \n    def forward(self, x):\n        x = x.unsqueeze(1)  # Add channel dim -> (B, C=1, D)\n        x = self.pool(torch.relu(self.conv1(x)))\n        x = x.view(x.size(0), -1)\n        return self.fc(x)\n\nX_ft_train_tensor = torch.tensor(X_ft_train, dtype=torch.float32)\nX_ft_test_tensor = torch.tensor(X_ft_test, dtype=torch.float32)\ny_ft_train_tensor = torch.tensor(y_ft_train, dtype=torch.long)\ny_ft_test_tensor = torch.tensor(y_ft_test, dtype=torch.long)\n\ntrain_ft_ds = TensorDataset(X_ft_train_tensor, y_ft_train_tensor)\ntest_ft_ds = TensorDataset(X_ft_test_tensor, y_ft_test_tensor)\n\ntrain_ft_loader = DataLoader(train_ft_ds, batch_size=64, shuffle=True)\ntest_ft_loader = DataLoader(test_ft_ds, batch_size=64)\n\nmodel_ft = CNNFT()\noptimizer = torch.optim.Adam(model_ft.parameters(), lr=1e-3)\ncriterion = nn.CrossEntropyLoss()\n\n# Train loop (simple)\nfor epoch in range(5):\n    model_ft.train()\n    total_loss = 0\n    for xb, yb in train_ft_loader:\n        optimizer.zero_grad()\n        pred = model_ft(xb)\n        loss = criterion(pred, yb)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    \n    avg_loss = total_loss / len(train_ft_loader)\n    print(f\"Epoch {epoch+1} - Loss: {avg_loss:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T10:31:15.462809Z","iopub.execute_input":"2025-06-23T10:31:15.463380Z","iopub.status.idle":"2025-06-23T10:31:40.014508Z","shell.execute_reply.started":"2025-06-23T10:31:15.463354Z","shell.execute_reply":"2025-06-23T10:31:40.013657Z"}},"outputs":[{"name":"stdout","text":"Epoch 1 - Loss: 0.3603\nEpoch 2 - Loss: 0.2865\nEpoch 3 - Loss: 0.2738\nEpoch 4 - Loss: 0.2654\nEpoch 5 - Loss: 0.2615\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"def evaluate_model(model, dataloader):\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for xb, yb in dataloader:\n            outputs = model(xb)\n            _, predicted = torch.max(outputs, 1)\n            correct += (predicted == yb).sum().item()\n            total += yb.size(0)\n    return correct / total\n\naccuracy = evaluate_model(model_ft, test_ft_loader)\nprint(f\"FastText + CNN Test Accuracy: {accuracy * 100:.2f}%\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T10:32:03.109741Z","iopub.execute_input":"2025-06-23T10:32:03.110043Z","iopub.status.idle":"2025-06-23T10:32:03.572825Z","shell.execute_reply.started":"2025-06-23T10:32:03.110021Z","shell.execute_reply":"2025-06-23T10:32:03.572067Z"}},"outputs":[{"name":"stdout","text":"FastText + CNN Test Accuracy: 89.56%\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"#  BERT (Transformer) from HuggingFace","metadata":{}},{"cell_type":"code","source":"from transformers import BertTokenizer, BertModel\nfrom transformers import BertForSequenceClassification, Trainer, TrainingArguments\nfrom datasets import Dataset\n\n# Load BERT tokenizer\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n\n# Tokenize data\ntrain_texts, test_texts, train_labels, test_labels = train_test_split(df[\"text\"].tolist(), y.tolist(), test_size=0.2)\n\ntrain_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128)\ntest_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=128)\n\ntrain_dataset = Dataset.from_dict({\n    \"input_ids\": train_encodings[\"input_ids\"],\n    \"attention_mask\": train_encodings[\"attention_mask\"],\n    \"labels\": train_labels\n})\n\ntest_dataset = Dataset.from_dict({\n    \"input_ids\": test_encodings[\"input_ids\"],\n    \"attention_mask\": test_encodings[\"attention_mask\"],\n    \"labels\": test_labels\n})\n\n# Define model\nmodel_bert = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n\n# Trainer setup\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=2,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    evaluation_strategy=\"epoch\",\n    logging_dir=\"./logs\",\n    logging_steps=10,\n)\n\ntrainer = Trainer(\n    model=model_bert,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset\n)\n\n# Train BERT\ntrainer.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T10:40:10.066228Z","iopub.execute_input":"2025-06-23T10:40:10.066582Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}