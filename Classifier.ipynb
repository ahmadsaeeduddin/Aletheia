{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General-purpose\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Torch for modeling\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Scikit-learn for preprocessing and evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# NLP tools\n",
    "import spacy\n",
    "from textblob import TextBlob\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "\n",
    "# Progress bar\n",
    "from tqdm import tqdm, trange\n",
    "import time\n",
    "\n",
    "# Word cloud for visualization\n",
    "from wordcloud import WordCloud\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Good things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>Label</th>\n",
       "      <th>text_length</th>\n",
       "      <th>word_count</th>\n",
       "      <th>title_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>donald trump sends out embarrassing new year e...</td>\n",
       "      <td>donald trump just could not wish all american ...</td>\n",
       "      <td>0</td>\n",
       "      <td>2283</td>\n",
       "      <td>385</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>drunk bragging trump staffer started russian c...</td>\n",
       "      <td>house intelligence committee chairman devin nu...</td>\n",
       "      <td>0</td>\n",
       "      <td>1673</td>\n",
       "      <td>248</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sheriff david clarke becomes internet joke for...</td>\n",
       "      <td>friday wa revealed that former milwaukee sheri...</td>\n",
       "      <td>0</td>\n",
       "      <td>2643</td>\n",
       "      <td>422</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>trump obsessed even ha obama name coded into h...</td>\n",
       "      <td>christmas day donald trump announced that woul...</td>\n",
       "      <td>0</td>\n",
       "      <td>2095</td>\n",
       "      <td>338</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pope francis just called out donald trump duri...</td>\n",
       "      <td>pope francis used his annual christmas day mes...</td>\n",
       "      <td>0</td>\n",
       "      <td>1990</td>\n",
       "      <td>332</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  donald trump sends out embarrassing new year e...   \n",
       "1  drunk bragging trump staffer started russian c...   \n",
       "2  sheriff david clarke becomes internet joke for...   \n",
       "3  trump obsessed even ha obama name coded into h...   \n",
       "4  pope francis just called out donald trump duri...   \n",
       "\n",
       "                                                text  Label  text_length  \\\n",
       "0  donald trump just could not wish all american ...      0         2283   \n",
       "1  house intelligence committee chairman devin nu...      0         1673   \n",
       "2  friday wa revealed that former milwaukee sheri...      0         2643   \n",
       "3  christmas day donald trump announced that woul...      0         2095   \n",
       "4  pope francis used his annual christmas day mes...      0         1990   \n",
       "\n",
       "   word_count  title_length  \n",
       "0         385            72  \n",
       "1         248            68  \n",
       "2         422            78  \n",
       "3         338            62  \n",
       "4         332            69  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dff = pd.read_csv('final_data.csv')\n",
    "dff.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting en-core-web-md==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.8.0/en_core_web_md-3.8.0-py3-none-any.whl (33.5 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 33.5/33.5 MB 28.4 MB/s eta 0:00:00\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_md')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "spacy.cli.download(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download necessary models\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "# nltk.download('vader_lexicon')\n",
    "sid = SentimentIntensityAnalyzer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device -> cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53f718b021184986bb40b18dfd55510d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/3111 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/lab/Desktop/Genesys-Lab-Internship/Classifier.ipynb Cell 8\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/lab/Desktop/Genesys-Lab-Internship/Classifier.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mDevice -> \u001b[39m\u001b[39m{\u001b[39;00mdevice\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/lab/Desktop/Genesys-Lab-Internship/Classifier.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m model \u001b[39m=\u001b[39m SentenceTransformer(\u001b[39m'\u001b[39m\u001b[39mall-MiniLM-L6-v2\u001b[39m\u001b[39m'\u001b[39m, device\u001b[39m=\u001b[39mdevice)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/lab/Desktop/Genesys-Lab-Internship/Classifier.ipynb#X10sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m text_embeddings \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mencode(dff[\u001b[39m'\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mtolist(), show_progress_bar\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m, convert_to_tensor\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, device\u001b[39m=\u001b[39;49mdevice)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:685\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    682\u001b[0m features\u001b[39m.\u001b[39mupdate(extra_features)\n\u001b[1;32m    684\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 685\u001b[0m     out_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(features, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    686\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice\u001b[39m.\u001b[39mtype \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhpu\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    687\u001b[0m         out_features \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(out_features)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:758\u001b[0m, in \u001b[0;36mSentenceTransformer.forward\u001b[0;34m(self, input, **kwargs)\u001b[0m\n\u001b[1;32m    756\u001b[0m     module_kwarg_keys \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule_kwargs\u001b[39m.\u001b[39mget(module_name, [])\n\u001b[1;32m    757\u001b[0m     module_kwargs \u001b[39m=\u001b[39m {key: value \u001b[39mfor\u001b[39;00m key, value \u001b[39min\u001b[39;00m kwargs\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m key \u001b[39min\u001b[39;00m module_kwarg_keys}\n\u001b[0;32m--> 758\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodule_kwargs)\n\u001b[1;32m    759\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1764\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sentence_transformers/models/Transformer.py:442\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, features, **kwargs)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Returns token_embeddings, cls_token\"\"\"\u001b[39;00m\n\u001b[1;32m    436\u001b[0m trans_features \u001b[39m=\u001b[39m {\n\u001b[1;32m    437\u001b[0m     key: value\n\u001b[1;32m    438\u001b[0m     \u001b[39mfor\u001b[39;00m key, value \u001b[39min\u001b[39;00m features\u001b[39m.\u001b[39mitems()\n\u001b[1;32m    439\u001b[0m     \u001b[39mif\u001b[39;00m key \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtoken_type_ids\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39minputs_embeds\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    440\u001b[0m }\n\u001b[0;32m--> 442\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mauto_model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtrans_features, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs, return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    443\u001b[0m token_embeddings \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    444\u001b[0m features[\u001b[39m\"\u001b[39m\u001b[39mtoken_embeddings\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m token_embeddings\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1764\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1016\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1009\u001b[0m \u001b[39m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m   1010\u001b[0m \u001b[39m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m   1011\u001b[0m \u001b[39m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m   1012\u001b[0m \u001b[39m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m   1013\u001b[0m \u001b[39m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m   1014\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m-> 1016\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m   1017\u001b[0m     embedding_output,\n\u001b[1;32m   1018\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m   1019\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1020\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1021\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m   1022\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1023\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1024\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1025\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1026\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1027\u001b[0m )\n\u001b[1;32m   1028\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1029\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1764\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:662\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    651\u001b[0m     layer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    652\u001b[0m         layer_module\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m,\n\u001b[1;32m    653\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    659\u001b[0m         output_attentions,\n\u001b[1;32m    660\u001b[0m     )\n\u001b[1;32m    661\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 662\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    663\u001b[0m         hidden_states,\n\u001b[1;32m    664\u001b[0m         attention_mask,\n\u001b[1;32m    665\u001b[0m         layer_head_mask,\n\u001b[1;32m    666\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    667\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    668\u001b[0m         past_key_value,\n\u001b[1;32m    669\u001b[0m         output_attentions,\n\u001b[1;32m    670\u001b[0m     )\n\u001b[1;32m    672\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    673\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1764\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:594\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    591\u001b[0m     cross_attn_present_key_value \u001b[39m=\u001b[39m cross_attention_outputs[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    592\u001b[0m     present_key_value \u001b[39m=\u001b[39m present_key_value \u001b[39m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 594\u001b[0m layer_output \u001b[39m=\u001b[39m apply_chunking_to_forward(\n\u001b[1;32m    595\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeed_forward_chunk, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchunk_size_feed_forward, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mseq_len_dim, attention_output\n\u001b[1;32m    596\u001b[0m )\n\u001b[1;32m    597\u001b[0m outputs \u001b[39m=\u001b[39m (layer_output,) \u001b[39m+\u001b[39m outputs\n\u001b[1;32m    599\u001b[0m \u001b[39m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/pytorch_utils.py:253\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[39m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    251\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat(output_chunks, dim\u001b[39m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 253\u001b[0m \u001b[39mreturn\u001b[39;00m forward_fn(\u001b[39m*\u001b[39;49minput_tensors)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:607\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    605\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfeed_forward_chunk\u001b[39m(\u001b[39mself\u001b[39m, attention_output):\n\u001b[1;32m    606\u001b[0m     intermediate_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintermediate(attention_output)\n\u001b[0;32m--> 607\u001b[0m     layer_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput(intermediate_output, attention_output)\n\u001b[1;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1764\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:519\u001b[0m, in \u001b[0;36mBertOutput.forward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states: torch\u001b[39m.\u001b[39mTensor, input_tensor: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m--> 519\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdense(hidden_states)\n\u001b[1;32m    520\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(hidden_states)\n\u001b[1;32m    521\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mLayerNorm(hidden_states \u001b[39m+\u001b[39m input_tensor)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1764\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Device -> {device}')\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n",
    "\n",
    "text_embeddings = model.encode(dff['text'].tolist(), show_progress_bar=True, batch_size=32, convert_to_tensor=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_48617/1756431424.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'text_embeddings': torch.tensor(text_embeddings)\n"
     ]
    }
   ],
   "source": [
    "torch.save({\n",
    "    'text_embeddings': torch.tensor(text_embeddings)\n",
    "}, 'embeddings.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just for loading The text_embeddings\n",
    "\n",
    "checkpoint = torch.load('embeddings.pth')\n",
    "\n",
    "# Retrieve embeddings\n",
    "text_embeddings = checkpoint['text_embeddings']\n",
    "\n",
    "text_embeddings = text_embeddings.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🔍 Preprocessing articles:  42%|████▏     | 41994/99531 [37:16<1:09:08, 13.87it/s] "
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Sentence-level Preprocessing ---\n",
    "\n",
    "def preprocess_article(text):\n",
    "    doc = nlp(text)\n",
    "    processed_sentences = []\n",
    "    for sent in doc.sents:\n",
    "        s = sent.text.strip()\n",
    "        if len(s) > 1:\n",
    "            processed_sentences.append({\n",
    "                \"original\": s,\n",
    "                \"word_count\": len(s.split())\n",
    "            })\n",
    "    return processed_sentences\n",
    "\n",
    "# --- Feature Functions ---\n",
    "\n",
    "def paraphrasing_rate(processed_sentences, T=0.8):\n",
    "    embeddings = [nlp(s['original']).vector for s in processed_sentences]\n",
    "    N = len(embeddings)\n",
    "    if N <= 1:\n",
    "        return 0.0\n",
    "    count = 0\n",
    "    for i in range(N):\n",
    "        sims = [\n",
    "            cosine_similarity([embeddings[i]], [embeddings[j]])[0][0]\n",
    "            for j in range(N) if j != i\n",
    "        ]\n",
    "        if max(sims, default=0) > T:\n",
    "            count += 1\n",
    "    return count / N\n",
    "\n",
    "def subjectivity_ratio(processed_sentences):\n",
    "    N = len(processed_sentences)\n",
    "    if N == 0:\n",
    "        return 0.0\n",
    "    subj = sum(1 for s in processed_sentences\n",
    "               if TextBlob(s['original']).sentiment.subjectivity > 0.5)\n",
    "    return subj / N\n",
    "\n",
    "def sentiment_intensity_ratio(processed_sentences):\n",
    "    N = len(processed_sentences)\n",
    "    if N == 0:\n",
    "        return 0.0\n",
    "    intense = sum(1 for s in processed_sentences\n",
    "                  if abs(sid.polarity_scores(s['original'])['compound']) > 0.5)\n",
    "    return intense / N\n",
    "\n",
    "def average_sentence_length(processed_sentences):\n",
    "    N = len(processed_sentences)\n",
    "    if N == 0:\n",
    "        return 0.0\n",
    "    total = sum(s['word_count'] for s in processed_sentences)\n",
    "    return total / N\n",
    "\n",
    "def manipulative_score(processed_sentences, alpha=0.5, beta=0.5):\n",
    "    return alpha * subjectivity_ratio(processed_sentences) + beta * sentiment_intensity_ratio(processed_sentences)\n",
    "\n",
    "# --- Dataset Preparation ---\n",
    "\n",
    "def prepare_dataset(df, text_vectors):\n",
    "    X, y = [], []\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"🔍 Preprocessing articles\"):\n",
    "        text = row['text']\n",
    "        label = row['Label']\n",
    "        vec = text_vectors[idx]  # already extracted 384-d vector\n",
    "\n",
    "        processed = preprocess_article(text)\n",
    "        f1 = paraphrasing_rate(processed)\n",
    "        f2 = subjectivity_ratio(processed)\n",
    "        f3 = sentiment_intensity_ratio(processed)\n",
    "        f4 = average_sentence_length(processed)\n",
    "        f5 = manipulative_score(processed)\n",
    "\n",
    "        X.append(list(vec) + [f1, f2, f3, f4, f5])\n",
    "        y.append(label)\n",
    "\n",
    "    return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "# --- Model Definition ---\n",
    "\n",
    "class FakeNewsClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=389):\n",
    "        super(FakeNewsClassifier, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.net(x)).squeeze(1)\n",
    "\n",
    "# --- Training and Evaluation ---\n",
    "\n",
    "def train(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    loop = tqdm(loader, desc=\"🛠️ Training\", leave=False)\n",
    "    for xb, yb in loop:\n",
    "        xb, yb = xb.to(device), yb.to(device).float()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(xb)\n",
    "        loss = criterion(out, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            preds = (model(xb) > 0.5).long()\n",
    "            correct += (preds == yb).sum().item()\n",
    "            total += yb.size(0)\n",
    "    return correct / total\n",
    "\n",
    "# --- Main Training Loop ---\n",
    "\n",
    "# Replace with your real data\n",
    "# dff = DataFrame with columns ['text', 'Label']\n",
    "# text_embeddings = Numpy array with shape (N, 384)\n",
    "\n",
    "X, y = prepare_dataset(dff, text_embeddings)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=32)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = FakeNewsClassifier().to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "epochs = 5\n",
    "best_acc = 0\n",
    "\n",
    "print(\"🏁 Starting training...\\n\")\n",
    "\n",
    "for epoch in trange(epochs, desc=\"📊 Epoch Progress\"):\n",
    "    start = time.time()\n",
    "\n",
    "    loss = train(model, train_loader, criterion, optimizer, device)\n",
    "    acc = evaluate(model, test_loader, device)\n",
    "\n",
    "    elapsed = time.time() - start\n",
    "    print(f\"📅 Epoch {epoch+1}/{epochs} — 🧠 Loss: {loss:.4f} | 🎯 Test Acc: {acc:.4f} | ⏱️ Time: {elapsed:.2f}s\")\n",
    "\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        torch.save(model.state_dict(), \"best_fake_news.pt\")\n",
    "        print(\"✅ New best model saved!\")\n",
    "\n",
    "print(f\"\\n✅ Training done. Best test accuracy: {best_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
