{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/lab/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/lab/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/lab/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataset: (99531, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>Label</th>\n",
       "      <th>text_length</th>\n",
       "      <th>word_count</th>\n",
       "      <th>title_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>donald trump sends out embarrassing new year e...</td>\n",
       "      <td>donald trump just could not wish all american ...</td>\n",
       "      <td>0</td>\n",
       "      <td>2283</td>\n",
       "      <td>385</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>drunk bragging trump staffer started russian c...</td>\n",
       "      <td>house intelligence committee chairman devin nu...</td>\n",
       "      <td>0</td>\n",
       "      <td>1673</td>\n",
       "      <td>248</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sheriff david clarke becomes internet joke for...</td>\n",
       "      <td>friday wa revealed that former milwaukee sheri...</td>\n",
       "      <td>0</td>\n",
       "      <td>2643</td>\n",
       "      <td>422</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>trump obsessed even ha obama name coded into h...</td>\n",
       "      <td>christmas day donald trump announced that woul...</td>\n",
       "      <td>0</td>\n",
       "      <td>2095</td>\n",
       "      <td>338</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pope francis just called out donald trump duri...</td>\n",
       "      <td>pope francis used his annual christmas day mes...</td>\n",
       "      <td>0</td>\n",
       "      <td>1990</td>\n",
       "      <td>332</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  donald trump sends out embarrassing new year e...   \n",
       "1  drunk bragging trump staffer started russian c...   \n",
       "2  sheriff david clarke becomes internet joke for...   \n",
       "3  trump obsessed even ha obama name coded into h...   \n",
       "4  pope francis just called out donald trump duri...   \n",
       "\n",
       "                                                text  Label  text_length  \\\n",
       "0  donald trump just could not wish all american ...      0         2283   \n",
       "1  house intelligence committee chairman devin nu...      0         1673   \n",
       "2  friday wa revealed that former milwaukee sheri...      0         2643   \n",
       "3  christmas day donald trump announced that woul...      0         2095   \n",
       "4  pope francis used his annual christmas day mes...      0         1990   \n",
       "\n",
       "   word_count  title_length  \n",
       "0         385            72  \n",
       "1         248            68  \n",
       "2         422            78  \n",
       "3         338            62  \n",
       "4         332            69  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chunk 1: Environment Setup and Data Loading\n",
    "\n",
    "# --- 1.1: Install required libraries (uncomment if running first time) ---\n",
    "# !pip install pandas numpy sklearn matplotlib seaborn nltk tqdm\n",
    "\n",
    "# --- 1.2: Imports ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# NLTK setup\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# --- 1.3: Load Dataset ---\n",
    "# Replace with your actual path if local file is available\n",
    "try:\n",
    "    df = pd.read_csv('final_data.csv')\n",
    "except FileNotFoundError:\n",
    "    # Try loading from an online source (fallback)\n",
    "    df = pd.read_csv(\"https://raw.githubusercontent.com/diptamath/covid_fake_news/main/data/welfake.csv\")\n",
    "\n",
    "# --- 1.4: Quick View of Dataset ---\n",
    "print(\"Shape of dataset:\", df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before preprocessing:\n",
      "donald trump sends out embarrassing new year eve message this disturbing donald trump just could not wish all american happy new year and leave that instead had give shout out his enemy hater and the very dishonest fake news medium the former reality show star had just one job and could not our country rapidly grows stronger and smarter want wish all friend supporter enemy hater and even the very dishonest fake news medium happy and healthy new year president angry pant tweeted 2018 will great year for america our country rapidly grows stronger and smarter want wish all friend supporter enemy hater and even the very dishonest fake news medium happy and healthy new year 2018 will great year for america donald trump december 2017trump tweet went down about welll you expect what kind president sends new year greeting like this despicable petty infantile gibberish only trump his lack decency will not even allow him rise above the gutter long enough wish the american citizen happy new year bishop talbert swan december 2017no one like you calvin december 2017your impeachment would make 2018 great year for america but will also accept regaining control congress miranda yaver december 2017do you hear yourself talk when you have include that many people that hate you you have wonder why the they all hate alan sandoval december 2017who us the word hater new year wish marlene december 2017you can not just say happy new year koren pollitt december 2017here trump new year eve tweet from 2016. happy new year all including many enemy and those who have fought and lost badly they just not know what love donald trump december 2016this nothing new for trump been doing this for year trump ha directed message his enemy and hater for new year easter thanksgiving and the anniversary 11. daniel dale december 2017trump holiday tweet are clearly not presidential how long did work hallmark before becoming president steven goodine december 2017he always been like this the only difference that the last few year his filter ha been breaking down roy schulze december 2017who apart from teenager us the term hater wendy december 2017he fucking year old who know december 2017so all the people who voted for this hole thinking would change once got into power you were wrong 70-year-old men not change and now year older photo andrew burton getty image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99531/99531 [01:52<00:00, 887.34it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After preprocessing:\n",
      "donald trump sends embarrassing new year eve message disturbing donald trump could not wish american happy new year leave instead give shout his enemy hater dishonest fake news medium former reality show star one job could not country rapidly grows stronger smarter want wish friend supporter enemy hater even dishonest fake news medium happy healthy new year president angry pant tweeted will great year america country rapidly grows stronger smarter want wish friend supporter enemy hater even dishonest fake news medium happy healthy new year will great year america donald trump december trump tweet went welll you expect kind president sends new year greeting like despicable petty infantile gibberish trump his lack decency will not even allow rise gutter long enough wish american citizen happy new year bishop talbert swan december no one like you calvin december your impeachment would make great year america will also accept regaining control congress miranda yaver december you hear talk you include many people hate you you wonder they hate alan sandoval december u word hater new year wish marlene december you not say happy new year koren pollitt december trump new year eve tweet happy new year including many enemy fought lost badly they not know love donald trump december nothing new trump year trump ha directed message his enemy hater new year easter thanksgiving anniversary daniel dale december trump holiday tweet are clearly not presidential long work hallmark becoming president steven goodine december he always like difference last year his filter ha breaking roy schulze december apart teenager u term hater wendy december he fucking year old know december people voted hole thinking would change got power you wrong yearold men not change year older photo andrew burton getty image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Chunk 2: Data Preprocessing\n",
    "\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "# --- 2.1: Text Cleaning Function ---\n",
    "def clean_text(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Remove extra whitespaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# --- 2.2: Stopword Removal and Lemmatization ---\n",
    "# Define custom list of important words to preserve\n",
    "important_words = {\n",
    "    'not', 'no', 'never', 'nothing', 'nowhere', 'none', 'nobody',\n",
    "    'would', 'could', 'should', 'will', 'was', 'is', 'are',\n",
    "    'you', 'we', 'he', 'they', 'your', 'his', 'her', 'their'\n",
    "}\n",
    "\n",
    "# Load NLTK stopwords and remove important words\n",
    "stop_words = set(stopwords.words('english')) - important_words\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Clean\n",
    "    text = clean_text(text)\n",
    "    # Tokenize\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # Remove stopwords and lemmatize\n",
    "    processed = [lemmatizer.lemmatize(w) for w in tokens if w not in stop_words]\n",
    "    return ' '.join(processed)\n",
    "\n",
    "# --- 2.3: Apply Preprocessing to the Dataset ---\n",
    "\n",
    "# Fill missing text/title values with empty strings (handles NaN)\n",
    "if {'title', 'text'}.issubset(df.columns):\n",
    "    df['content'] = df['title'].fillna('') + ' ' + df['text'].fillna('')\n",
    "else:\n",
    "    df['content'] = df['text'].fillna('')\n",
    "\n",
    "# Drop duplicate content rows if any\n",
    "df.drop_duplicates(subset='content', inplace=True)\n",
    "\n",
    "# Show a sample before preprocessing\n",
    "print(\"Before preprocessing:\")\n",
    "print(df['content'].iloc[0])\n",
    "\n",
    "# Add tqdm progress bar for preprocessing\n",
    "tqdm.pandas()\n",
    "df['processed'] = df['content'].progress_apply(preprocess_text)\n",
    "\n",
    "# Show the result\n",
    "print(\"\\nAfter preprocessing:\")\n",
    "print(df['processed'].iloc[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting LFS: 100%|██████████| 99531/99531 [37:46<00:00, 43.92it/s]  \n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'label'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3811\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3812\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3813\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32mpandas/_libs/index.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/index.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'label'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/lab/Documents/i221963/Linguistic Features.ipynb Cell 4\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/lab/Documents/i221963/Linguistic%20Features.ipynb#W3sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m lf_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(linguistic_features, columns\u001b[39m=\u001b[39mlf_columns)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/lab/Documents/i221963/Linguistic%20Features.ipynb#W3sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m \u001b[39m# --- 3.4: Feature selection using Pearson correlation (via ANOVA F-test as proxy) ---\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/lab/Documents/i221963/Linguistic%20Features.ipynb#W3sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m y \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39;49m\u001b[39mlabel\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39mastype(\u001b[39mint\u001b[39m)  \u001b[39m# Ensure labels are integers\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/lab/Documents/i221963/Linguistic%20Features.ipynb#W3sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m selector \u001b[39m=\u001b[39m SelectKBest(score_func\u001b[39m=\u001b[39mf_classif, k\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mall\u001b[39m\u001b[39m'\u001b[39m)  \u001b[39m# We'll filter manually\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/lab/Documents/i221963/Linguistic%20Features.ipynb#W3sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m selector\u001b[39m.\u001b[39mfit(lf_df, y)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/frame.py:4107\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4105\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   4106\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4107\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   4108\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4109\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:3819\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3814\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(casted_key, \u001b[39mslice\u001b[39m) \u001b[39mor\u001b[39;00m (\n\u001b[1;32m   3815\u001b[0m         \u001b[39misinstance\u001b[39m(casted_key, abc\u001b[39m.\u001b[39mIterable)\n\u001b[1;32m   3816\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39many\u001b[39m(\u001b[39misinstance\u001b[39m(x, \u001b[39mslice\u001b[39m) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m casted_key)\n\u001b[1;32m   3817\u001b[0m     ):\n\u001b[1;32m   3818\u001b[0m         \u001b[39mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3819\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3820\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3821\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3822\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3823\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3824\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'label'"
     ]
    }
   ],
   "source": [
    "# Chunk 3: Linguistic Feature Extraction (LFS)\n",
    "\n",
    "import textstat\n",
    "import spacy\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# --- 3.1: Function to extract linguistic features ---\n",
    "def extract_linguistic_features(text):\n",
    "    doc = nlp(text)\n",
    "    num_words = len([token for token in doc if token.is_alpha])\n",
    "    num_chars = sum(len(token.text) for token in doc if token.is_alpha)\n",
    "    avg_word_length = np.mean([len(token.text) for token in doc if token.is_alpha]) if num_words > 0 else 0\n",
    "    num_sentences = len(list(doc.sents))\n",
    "    num_nouns = len([token for token in doc if token.pos_ == \"NOUN\"])\n",
    "    num_verbs = len([token for token in doc if token.pos_ == \"VERB\"])\n",
    "    num_adj = len([token for token in doc if token.pos_ == \"ADJ\"])\n",
    "    num_adv = len([token for token in doc if token.pos_ == \"ADV\"])\n",
    "    num_entities = len(doc.ents)\n",
    "    flesch_read = textstat.flesch_reading_ease(text)\n",
    "\n",
    "    return [\n",
    "        num_words,\n",
    "        num_chars,\n",
    "        avg_word_length,\n",
    "        num_sentences,\n",
    "        num_nouns,\n",
    "        num_verbs,\n",
    "        num_adj,\n",
    "        num_adv,\n",
    "        num_entities,\n",
    "        flesch_read\n",
    "    ]\n",
    "\n",
    "# --- 3.2: Extract features for all rows ---\n",
    "linguistic_features = []\n",
    "for text in tqdm(df['processed'], desc=\"Extracting LFS\"):\n",
    "    linguistic_features.append(extract_linguistic_features(text))\n",
    "\n",
    "linguistic_features = np.array(linguistic_features)\n",
    "\n",
    "# --- 3.3: Convert to DataFrame ---\n",
    "lf_columns = [\n",
    "    'word_count', 'char_count', 'avg_word_length', 'sentence_count',\n",
    "    'noun_count', 'verb_count', 'adj_count', 'adv_count', 'ner_count', 'flesch_readability'\n",
    "]\n",
    "lf_df = pd.DataFrame(linguistic_features, columns=lf_columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature correlation scores:\n",
      "              feature       score\n",
      "5          verb_count  566.117891\n",
      "8           ner_count  512.139475\n",
      "0          word_count  459.640822\n",
      "9  flesch_readability  449.838762\n",
      "4          noun_count  426.845024\n",
      "1          char_count  411.270524\n",
      "7           adv_count  249.409724\n",
      "6           adj_count  248.462142\n",
      "3      sentence_count  114.577411\n",
      "2     avg_word_length   16.815967\n",
      "\n",
      "Sample LFS1:\n",
      "   verb_count  flesch_readability  adv_count  avg_word_length\n",
      "0   -0.195475            0.135675   0.189242        -1.993353\n",
      "1   -0.437571            0.332879  -0.357570         0.785218\n",
      "2    0.046620           -0.043916   0.006971        -1.382977\n",
      "3   -0.178183            0.193634   0.249998        -1.143381\n",
      "4   -0.057135            0.290332  -0.296813        -1.221956\n"
     ]
    }
   ],
   "source": [
    "# --- 3.4: Feature selection using Pearson correlation (via ANOVA F-test as proxy) ---\n",
    "y = df['Label'].astype(int)  # Ensure labels are integers\n",
    "selector = SelectKBest(score_func=f_classif, k='all')  # We'll filter manually\n",
    "selector.fit(lf_df, y)\n",
    "scores = selector.scores_\n",
    "\n",
    "# Attach scores to features and sort\n",
    "feature_scores = pd.DataFrame({'feature': lf_columns, 'score': scores})\n",
    "feature_scores.sort_values(by='score', ascending=False, inplace=True)\n",
    "\n",
    "print(\"Feature correlation scores:\")\n",
    "print(feature_scores)\n",
    "\n",
    "# --- 3.5: Create 3 odd LFS sets for voting ---\n",
    "sorted_features = feature_scores['feature'].tolist()\n",
    "\n",
    "LFS1 = lf_df[[sorted_features[i] for i in range(0, len(sorted_features), 3)]]\n",
    "LFS2 = lf_df[[sorted_features[i] for i in range(1, len(sorted_features), 3)]]\n",
    "LFS3 = lf_df[[sorted_features[i] for i in range(2, len(sorted_features), 3)]]\n",
    "\n",
    "# Normalize feature sets\n",
    "scaler = StandardScaler()\n",
    "LFS1_scaled = pd.DataFrame(scaler.fit_transform(LFS1), columns=LFS1.columns)\n",
    "LFS2_scaled = pd.DataFrame(scaler.fit_transform(LFS2), columns=LFS2.columns)\n",
    "LFS3_scaled = pd.DataFrame(scaler.fit_transform(LFS3), columns=LFS3.columns)\n",
    "\n",
    "# Save for later use\n",
    "df_lfs1 = LFS1_scaled\n",
    "df_lfs2 = LFS2_scaled\n",
    "df_lfs3 = LFS3_scaled\n",
    "\n",
    "print(\"\\nSample LFS1:\")\n",
    "print(df_lfs1.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lab/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:470: ConvergenceWarning: lbfgs failed to converge after 300 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=300).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer Accuracy (P2): 0.5270\n",
      "TF-IDF Accuracy (P1): 0.5460\n",
      "Better Word Embedding: TFIDF\n"
     ]
    }
   ],
   "source": [
    "# Chunk 4: Word Embedding + Generate P1 and P2\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# --- 4.1: Prepare train/test split ---\n",
    "X = df['processed']\n",
    "y = df['Label'].astype(int)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- 4.2: Apply CountVectorizer (CV) ---\n",
    "cv_vectorizer = CountVectorizer(max_features=5000, ngram_range=(1,2))\n",
    "X_train_cv = cv_vectorizer.fit_transform(X_train)\n",
    "X_test_cv = cv_vectorizer.transform(X_test)\n",
    "\n",
    "# --- 4.3: Apply TF-IDF ---\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1,2))\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# --- 4.4: Train baseline classifier (Logistic Regression) ---\n",
    "lr_cv = LogisticRegression(max_iter=1000)\n",
    "lr_tfidf = LogisticRegression(max_iter=1000)\n",
    "\n",
    "lr_cv.fit(X_train_cv, y_train)\n",
    "lr_tfidf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# --- 4.5: Evaluate accuracy ---\n",
    "y_pred_cv = lr_cv.predict(X_test_cv)\n",
    "y_pred_tfidf = lr_tfidf.predict(X_test_tfidf)\n",
    "\n",
    "acc_cv = accuracy_score(y_test, y_pred_cv)\n",
    "acc_tfidf = accuracy_score(y_test, y_pred_tfidf)\n",
    "\n",
    "print(f\"CountVectorizer Accuracy (P2): {acc_cv:.4f}\")\n",
    "print(f\"TF-IDF Accuracy (P1): {acc_tfidf:.4f}\")\n",
    "\n",
    "# --- 4.6: Decide better embedding ---\n",
    "best_embedding = 'cv' if acc_cv > acc_tfidf else 'tfidf'\n",
    "print(f\"Better Word Embedding: {best_embedding.upper()}\")\n",
    "\n",
    "# Save P1 and P2\n",
    "P1 = y_pred_tfidf\n",
    "P2 = y_pred_cv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk 5: Combine Best WE with LFS1, LFS2, LFS3 → Generate P3, P4, P5\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def get_best_model(X_train, X_test, y_train, y_test):\n",
    "    models = {\n",
    "        'SVM': SVC(kernel='linear', C=100, gamma=0.0001, probability=True),\n",
    "        'NB': MultinomialNB(alpha=1.0, fit_prior=True),\n",
    "        'KNN': KNeighborsClassifier(n_neighbors=2, weights='uniform'),\n",
    "        'DT': DecisionTreeClassifier(max_features=None, criterion='gini', ccp_alpha=0.02),\n",
    "        'Bagging': BaggingClassifier(n_estimators=100, bootstrap=True),\n",
    "        'AdaBoost': AdaBoostClassifier(n_estimators=50, learning_rate=1)\n",
    "    }\n",
    "\n",
    "    best_acc = 0\n",
    "    best_pred = None\n",
    "    best_name = None\n",
    "    for name, model in models.items():\n",
    "        # Use StandardScaler if KNN\n",
    "        if name == 'KNN':\n",
    "            pipeline = make_pipeline(StandardScaler(with_mean=False), model)\n",
    "            pipeline.fit(X_train, y_train)\n",
    "            y_pred = pipeline.predict(X_test)\n",
    "        else:\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_pred = y_pred\n",
    "            best_name = name\n",
    "\n",
    "    return best_pred, best_name, best_acc\n",
    "\n",
    "\n",
    "# 5.1: Re-split linguistic features to match text indices\n",
    "X_train_text, X_test_text, lfs_train, lfs_test = train_test_split(\n",
    "    df['processed'], lf_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Use only the best embedding\n",
    "if best_embedding == 'cv':\n",
    "    X_embed_train = cv_vectorizer.transform(X_train_text)\n",
    "    X_embed_test = cv_vectorizer.transform(X_test_text)\n",
    "else:\n",
    "    X_embed_train = tfidf_vectorizer.transform(X_train_text)\n",
    "    X_embed_test = tfidf_vectorizer.transform(X_test_text)\n",
    "\n",
    "# Prepare predictions P3, P4, P5\n",
    "P = []\n",
    "best_models = []\n",
    "\n",
    "for i, lfs in enumerate([df_lfs1, df_lfs2, df_lfs3]):\n",
    "    # Align split with current LFS\n",
    "    lfs_train_part, lfs_test_part = train_test_split(lfs, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Combine LFS with embedding\n",
    "    from scipy.sparse import hstack\n",
    "    X_comb_train = hstack([X_embed_train, lfs_train_part])\n",
    "    X_comb_test = hstack([X_embed_test, lfs_test_part])\n",
    "\n",
    "    pred, best_model_name, best_model_acc = get_best_model(X_comb_train, X_comb_test, y_train, y_test)\n",
    "    print(f\"LFS{i+1} → Best Model: {best_model_name}, Accuracy: {best_model_acc:.4f}\")\n",
    "    P.append(pred)\n",
    "    best_models.append(best_model_name)\n",
    "\n",
    "P3, P4, P5 = P\n",
    "\n",
    "\n",
    "# Chunk 6: Hard Voting on P3, P4, P5 → Output P6\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import mode\n",
    "\n",
    "# Stack predictions from LFS1, LFS2, LFS3\n",
    "stacked_preds = np.vstack((P3, P4, P5)).T\n",
    "\n",
    "# Apply hard voting (mode)\n",
    "P6 = mode(stacked_preds, axis=1).mode.flatten()\n",
    "\n",
    "# Evaluation of LFS-Enabled Voting (P6)\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "print(\"Evaluation of LFS-Enabled WE Voting Output (P6):\")\n",
    "print(f\"Accuracy:  {accuracy_score(y_test, P6):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, P6):.4f}\")\n",
    "print(f\"Recall:    {recall_score(y_test, P6):.4f}\")\n",
    "print(f\"F1 Score:  {f1_score(y_test, P6):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk 7: Final Hard Voting across P1 (TF-IDF), P2 (CV), and P6 (LFS Voting)\n",
    "\n",
    "# Stack P1, P2, P6 predictions\n",
    "final_stacked = np.vstack((P1, P2, P6)).T\n",
    "\n",
    "# Apply final hard voting (mode)\n",
    "final_prediction = mode(final_stacked, axis=1).mode.flatten()\n",
    "\n",
    "# Final Evaluation of WELFake Model\n",
    "print(\"Final Evaluation of WELFake (P1 + P2 + P6 Voting):\")\n",
    "print(f\"Accuracy:  {accuracy_score(y_test, final_prediction):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, final_prediction):.4f}\")\n",
    "print(f\"Recall:    {recall_score(y_test, final_prediction):.4f}\")\n",
    "print(f\"F1 Score:  {f1_score(y_test, final_prediction):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading GloVe embeddings...\n"
     ]
    },
    {
     "ename": "SSLError",
     "evalue": "HTTPSConnectionPool(host='downloads.cs.stanford.edu', port=443): Max retries exceeded with url: /nlp/data/glove.6B.zip (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:997)')))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSSLCertVerificationError\u001b[0m                  Traceback (most recent call last)",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/urllib3/connectionpool.py:704\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 704\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    705\u001b[0m     conn,\n\u001b[1;32m    706\u001b[0m     method,\n\u001b[1;32m    707\u001b[0m     url,\n\u001b[1;32m    708\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    709\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    710\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    711\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    712\u001b[0m )\n\u001b[1;32m    714\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    717\u001b[0m \u001b[39m# mess.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/urllib3/connectionpool.py:387\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 387\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_conn(conn)\n\u001b[1;32m    388\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    389\u001b[0m     \u001b[39m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/urllib3/connectionpool.py:1045\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1044\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mgetattr\u001b[39m(conn, \u001b[39m\"\u001b[39m\u001b[39msock\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):  \u001b[39m# AppEngine might not have  `.sock`\u001b[39;00m\n\u001b[0;32m-> 1045\u001b[0m     conn\u001b[39m.\u001b[39;49mconnect()\n\u001b[1;32m   1047\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m conn\u001b[39m.\u001b[39mis_verified:\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/urllib3/connection.py:414\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    412\u001b[0m     context\u001b[39m.\u001b[39mload_default_certs()\n\u001b[0;32m--> 414\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock \u001b[39m=\u001b[39m ssl_wrap_socket(\n\u001b[1;32m    415\u001b[0m     sock\u001b[39m=\u001b[39;49mconn,\n\u001b[1;32m    416\u001b[0m     keyfile\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkey_file,\n\u001b[1;32m    417\u001b[0m     certfile\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcert_file,\n\u001b[1;32m    418\u001b[0m     key_password\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkey_password,\n\u001b[1;32m    419\u001b[0m     ca_certs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mca_certs,\n\u001b[1;32m    420\u001b[0m     ca_cert_dir\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mca_cert_dir,\n\u001b[1;32m    421\u001b[0m     ca_cert_data\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mca_cert_data,\n\u001b[1;32m    422\u001b[0m     server_hostname\u001b[39m=\u001b[39;49mserver_hostname,\n\u001b[1;32m    423\u001b[0m     ssl_context\u001b[39m=\u001b[39;49mcontext,\n\u001b[1;32m    424\u001b[0m     tls_in_tls\u001b[39m=\u001b[39;49mtls_in_tls,\n\u001b[1;32m    425\u001b[0m )\n\u001b[1;32m    427\u001b[0m \u001b[39m# If we're using all defaults and the connection\u001b[39;00m\n\u001b[1;32m    428\u001b[0m \u001b[39m# is TLSv1 or TLSv1.1 we throw a DeprecationWarning\u001b[39;00m\n\u001b[1;32m    429\u001b[0m \u001b[39m# for the host.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/urllib3/util/ssl_.py:449\u001b[0m, in \u001b[0;36mssl_wrap_socket\u001b[0;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[39mif\u001b[39;00m send_sni:\n\u001b[0;32m--> 449\u001b[0m     ssl_sock \u001b[39m=\u001b[39m _ssl_wrap_socket_impl(\n\u001b[1;32m    450\u001b[0m         sock, context, tls_in_tls, server_hostname\u001b[39m=\u001b[39;49mserver_hostname\n\u001b[1;32m    451\u001b[0m     )\n\u001b[1;32m    452\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/urllib3/util/ssl_.py:493\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_impl\u001b[0;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[39mif\u001b[39;00m server_hostname:\n\u001b[0;32m--> 493\u001b[0m     \u001b[39mreturn\u001b[39;00m ssl_context\u001b[39m.\u001b[39;49mwrap_socket(sock, server_hostname\u001b[39m=\u001b[39;49mserver_hostname)\n\u001b[1;32m    494\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.10/ssl.py:513\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[0;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrap_socket\u001b[39m(\u001b[39mself\u001b[39m, sock, server_side\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    508\u001b[0m                 do_handshake_on_connect\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    509\u001b[0m                 suppress_ragged_eofs\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    510\u001b[0m                 server_hostname\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, session\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    511\u001b[0m     \u001b[39m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[1;32m    512\u001b[0m     \u001b[39m# ctx._wrap_socket()\u001b[39;00m\n\u001b[0;32m--> 513\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msslsocket_class\u001b[39m.\u001b[39;49m_create(\n\u001b[1;32m    514\u001b[0m         sock\u001b[39m=\u001b[39;49msock,\n\u001b[1;32m    515\u001b[0m         server_side\u001b[39m=\u001b[39;49mserver_side,\n\u001b[1;32m    516\u001b[0m         do_handshake_on_connect\u001b[39m=\u001b[39;49mdo_handshake_on_connect,\n\u001b[1;32m    517\u001b[0m         suppress_ragged_eofs\u001b[39m=\u001b[39;49msuppress_ragged_eofs,\n\u001b[1;32m    518\u001b[0m         server_hostname\u001b[39m=\u001b[39;49mserver_hostname,\n\u001b[1;32m    519\u001b[0m         context\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    520\u001b[0m         session\u001b[39m=\u001b[39;49msession\n\u001b[1;32m    521\u001b[0m     )\n",
      "File \u001b[0;32m/usr/lib/python3.10/ssl.py:1071\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[0;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[1;32m   1070\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1071\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdo_handshake()\n\u001b[1;32m   1072\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mOSError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n",
      "File \u001b[0;32m/usr/lib/python3.10/ssl.py:1342\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[0;34m(self, block)\u001b[0m\n\u001b[1;32m   1341\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msettimeout(\u001b[39mNone\u001b[39;00m)\n\u001b[0;32m-> 1342\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mdo_handshake()\n\u001b[1;32m   1343\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "\u001b[0;31mSSLCertVerificationError\u001b[0m: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:997)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    668\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    669\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    670\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    671\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    672\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    673\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    674\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    675\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    676\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    677\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    678\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    679\u001b[0m     )\n\u001b[1;32m    681\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/urllib3/connectionpool.py:788\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    786\u001b[0m     e \u001b[39m=\u001b[39m ProtocolError(\u001b[39m\"\u001b[39m\u001b[39mConnection aborted.\u001b[39m\u001b[39m\"\u001b[39m, e)\n\u001b[0;32m--> 788\u001b[0m retries \u001b[39m=\u001b[39m retries\u001b[39m.\u001b[39;49mincrement(\n\u001b[1;32m    789\u001b[0m     method, url, error\u001b[39m=\u001b[39;49me, _pool\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m, _stacktrace\u001b[39m=\u001b[39;49msys\u001b[39m.\u001b[39;49mexc_info()[\u001b[39m2\u001b[39;49m]\n\u001b[1;32m    790\u001b[0m )\n\u001b[1;32m    791\u001b[0m retries\u001b[39m.\u001b[39msleep()\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/urllib3/util/retry.py:592\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[39mif\u001b[39;00m new_retry\u001b[39m.\u001b[39mis_exhausted():\n\u001b[0;32m--> 592\u001b[0m     \u001b[39mraise\u001b[39;00m MaxRetryError(_pool, url, error \u001b[39mor\u001b[39;00m ResponseError(cause))\n\u001b[1;32m    594\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mIncremented Retry for (url=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m): \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m, url, new_retry)\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='downloads.cs.stanford.edu', port=443): Max retries exceeded with url: /nlp/data/glove.6B.zip (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:997)')))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mSSLError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/lab/Documents/i221963/Linguistic Features.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/lab/Documents/i221963/Linguistic%20Features.ipynb#X10sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(glove_dir):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/lab/Documents/i221963/Linguistic%20Features.ipynb#X10sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mDownloading GloVe embeddings...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/lab/Documents/i221963/Linguistic%20Features.ipynb#X10sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     r \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39;49mget(glove_url)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/lab/Documents/i221963/Linguistic%20Features.ipynb#X10sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(glove_zip, \u001b[39m\"\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/lab/Documents/i221963/Linguistic%20Features.ipynb#X10sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m         f\u001b[39m.\u001b[39mwrite(r\u001b[39m.\u001b[39mcontent)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(url, params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[39m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[39m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[39mreturn\u001b[39;00m request(\u001b[39m\"\u001b[39;49m\u001b[39mget\u001b[39;49m\u001b[39m\"\u001b[39;49m, url, params\u001b[39m=\u001b[39;49mparams, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/sessions.py:724\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[39mif\u001b[39;00m allow_redirects:\n\u001b[1;32m    722\u001b[0m     \u001b[39m# Redirect resolving generator.\u001b[39;00m\n\u001b[1;32m    723\u001b[0m     gen \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresolve_redirects(r, request, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 724\u001b[0m     history \u001b[39m=\u001b[39m [resp \u001b[39mfor\u001b[39;00m resp \u001b[39min\u001b[39;00m gen]\n\u001b[1;32m    725\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    726\u001b[0m     history \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/sessions.py:724\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[39mif\u001b[39;00m allow_redirects:\n\u001b[1;32m    722\u001b[0m     \u001b[39m# Redirect resolving generator.\u001b[39;00m\n\u001b[1;32m    723\u001b[0m     gen \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresolve_redirects(r, request, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 724\u001b[0m     history \u001b[39m=\u001b[39m [resp \u001b[39mfor\u001b[39;00m resp \u001b[39min\u001b[39;00m gen]\n\u001b[1;32m    725\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    726\u001b[0m     history \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/sessions.py:265\u001b[0m, in \u001b[0;36mSessionRedirectMixin.resolve_redirects\u001b[0;34m(self, resp, req, stream, timeout, verify, cert, proxies, yield_requests, **adapter_kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[39myield\u001b[39;00m req\n\u001b[1;32m    264\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 265\u001b[0m     resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(\n\u001b[1;32m    266\u001b[0m         req,\n\u001b[1;32m    267\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    268\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    269\u001b[0m         verify\u001b[39m=\u001b[39;49mverify,\n\u001b[1;32m    270\u001b[0m         cert\u001b[39m=\u001b[39;49mcert,\n\u001b[1;32m    271\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    272\u001b[0m         allow_redirects\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    273\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49madapter_kwargs,\n\u001b[1;32m    274\u001b[0m     )\n\u001b[1;32m    276\u001b[0m     extract_cookies_to_jar(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcookies, prepared_request, resp\u001b[39m.\u001b[39mraw)\n\u001b[1;32m    278\u001b[0m     \u001b[39m# extract redirect url, if any, for the next loop\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    705\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/adapters.py:698\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    694\u001b[0m         \u001b[39mraise\u001b[39;00m ProxyError(e, request\u001b[39m=\u001b[39mrequest)\n\u001b[1;32m    696\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e\u001b[39m.\u001b[39mreason, _SSLError):\n\u001b[1;32m    697\u001b[0m         \u001b[39m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[0;32m--> 698\u001b[0m         \u001b[39mraise\u001b[39;00m SSLError(e, request\u001b[39m=\u001b[39mrequest)\n\u001b[1;32m    700\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(e, request\u001b[39m=\u001b[39mrequest)\n\u001b[1;32m    702\u001b[0m \u001b[39mexcept\u001b[39;00m ClosedPoolError \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;31mSSLError\u001b[0m: HTTPSConnectionPool(host='downloads.cs.stanford.edu', port=443): Max retries exceeded with url: /nlp/data/glove.6B.zip (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:997)')))"
     ]
    }
   ],
   "source": [
    "# Chunk 8.1: Download GloVe embeddings (if not already downloaded)\n",
    "import os\n",
    "import zipfile\n",
    "import requests\n",
    "\n",
    "glove_dir = \"glove.6B\"\n",
    "glove_zip = \"glove.6B.zip\"\n",
    "glove_url = \"http://nlp.stanford.edu/data/glove.6B.zip\"\n",
    "\n",
    "if not os.path.exists(glove_dir):\n",
    "    print(\"Downloading GloVe embeddings...\")\n",
    "    r = requests.get(glove_url)\n",
    "    with open(glove_zip, \"wb\") as f:\n",
    "        f.write(r.content)\n",
    "    with zipfile.ZipFile(glove_zip, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(glove_dir)\n",
    "    print(\"GloVe download complete.\")\n",
    "else:\n",
    "    print(\"GloVe already exists.\")\n",
    "\n",
    "# Chunk 8.2: Prepare GloVe embedding matrix\n",
    "embedding_index = {}\n",
    "with open(f\"{glove_dir}/glove.6B.100d.txt\", encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coeffs = np.asarray(values[1:], dtype=\"float32\")\n",
    "        embedding_index[word] = coeffs\n",
    "\n",
    "embedding_dim = 100\n",
    "word_index = tokenizer_cv.word_index\n",
    "num_words = min(5000, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i < num_words:\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk 8.3: Define CNN architecture for text classification\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Conv1D, GlobalAveragePooling1D, Dense, Concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "input_layer = Input(shape=(max_seq_len,))\n",
    "embedding_layer = Embedding(input_dim=num_words,\n",
    "                            output_dim=embedding_dim,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=max_seq_len,\n",
    "                            trainable=False)(input_layer)\n",
    "\n",
    "# Three Conv1D layers with kernel sizes 2, 3, 4\n",
    "conv_2 = Conv1D(32, 2, activation=\"relu\")(embedding_layer)\n",
    "conv_3 = Conv1D(32, 3, activation=\"relu\")(embedding_layer)\n",
    "conv_4 = Conv1D(32, 4, activation=\"relu\")(embedding_layer)\n",
    "\n",
    "# Global average pooling\n",
    "pool_2 = GlobalAveragePooling1D()(conv_2)\n",
    "pool_3 = GlobalAveragePooling1D()(conv_3)\n",
    "pool_4 = GlobalAveragePooling1D()(conv_4)\n",
    "\n",
    "# Concatenate all pooled outputs\n",
    "concat = Concatenate()([pool_2, pool_3, pool_4])\n",
    "\n",
    "# Final Dense layers\n",
    "dense1 = Dense(64, activation=\"relu\")(concat)\n",
    "output = Dense(1, activation=\"sigmoid\")(dense1)\n",
    "\n",
    "cnn_model = Model(inputs=input_layer, outputs=output)\n",
    "cnn_model.compile(loss=\"binary_crossentropy\", optimizer=Adam(), metrics=[\"accuracy\"])\n",
    "cnn_model.summary()\n",
    "\n",
    "\n",
    "# Chunk 8.4: Train CNN model\n",
    "history_cnn = cnn_model.fit(X_train_seq, y_train,\n",
    "                            epochs=5,\n",
    "                            batch_size=32,\n",
    "                            validation_data=(X_test_seq, y_test))\n",
    "\n",
    "\n",
    "# Chunk 8.5: Evaluate CNN performance\n",
    "y_pred_cnn = (cnn_model.predict(X_test_seq) > 0.5).astype(int)\n",
    "\n",
    "print(\"CNN Evaluation Metrics:\")\n",
    "print(f\"Accuracy:  {accuracy_score(y_test, y_pred_cnn):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_cnn):.4f}\")\n",
    "print(f\"Recall:    {recall_score(y_test, y_pred_cnn):.4f}\")\n",
    "print(f\"F1 Score:  {f1_score(y_test, y_pred_cnn):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk 9.2: Load BERT tokenizer and encode dataset\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def bert_encode(texts, tokenizer, max_len=128):\n",
    "    return tokenizer(\n",
    "        list(texts),\n",
    "        max_length=max_len,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"tf\"\n",
    "    )\n",
    "\n",
    "X_train_bert = bert_encode(X_train, bert_tokenizer)\n",
    "X_test_bert = bert_encode(X_test, bert_tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lab/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/home/lab/Documents/i221963/Linguistic Features.ipynb Cell 10\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/lab/Documents/i221963/Linguistic%20Features.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Chunk 9.3: Build BERT model with 5 dense and 3 dropout layers\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/lab/Documents/i221963/Linguistic%20Features.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m TFBertModel\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/lab/Documents/i221963/Linguistic%20Features.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m Model\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/lab/Documents/i221963/Linguistic%20Features.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m \u001b[39mimport\u001b[39;00m Dense, Dropout, Input\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/lab/Documents/i221963/Linguistic%20Features.ipynb#X12sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m bert_model \u001b[39m=\u001b[39m TFBertModel\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mbert-base-uncased\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# Chunk 9.3: Build BERT model with 5 dense and 3 dropout layers\n",
    "from transformers import TFBertModel\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "\n",
    "bert_model = TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "input_ids = Input(shape=(128,), dtype=tf.int32, name=\"input_ids\")\n",
    "attention_mask = Input(shape=(128,), dtype=tf.int32, name=\"attention_mask\")\n",
    "\n",
    "bert_output = bert_model(input_ids, attention_mask=attention_mask)[1]  # [CLS] token\n",
    "\n",
    "x = Dense(256, activation=\"relu\")(bert_output)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(128, activation=\"relu\")(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(64, activation=\"relu\")(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(32, activation=\"relu\")(x)\n",
    "x = Dropout(0.2)(x)\n",
    "output = Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "final_bert_model = Model(inputs=[input_ids, attention_mask], outputs=output)\n",
    "\n",
    "final_bert_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "final_bert_model.summary()\n",
    "\n",
    "\n",
    "# Chunk 9.4: Train BERT model\n",
    "history_bert = final_bert_model.fit(\n",
    "    x={\"input_ids\": X_train_bert[\"input_ids\"], \"attention_mask\": X_train_bert[\"attention_mask\"]},\n",
    "    y=y_train,\n",
    "    validation_data=(\n",
    "        {\"input_ids\": X_test_bert[\"input_ids\"], \"attention_mask\": X_test_bert[\"attention_mask\"]},\n",
    "        y_test,\n",
    "    ),\n",
    "    epochs=3,\n",
    "    batch_size=16\n",
    ")\n",
    "\n",
    "# Chunk 9.5: Evaluate BERT model\n",
    "y_pred_bert_probs = final_bert_model.predict(\n",
    "    {\"input_ids\": X_test_bert[\"input_ids\"], \"attention_mask\": X_test_bert[\"attention_mask\"]}\n",
    ")\n",
    "y_pred_bert = (y_pred_bert_probs > 0.5).astype(int)\n",
    "\n",
    "print(\"BERT Evaluation Metrics:\")\n",
    "print(f\"Accuracy:  {accuracy_score(y_test, y_pred_bert):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_bert):.4f}\")\n",
    "print(f\"Recall:    {recall_score(y_test, y_pred_bert):.4f}\")\n",
    "print(f\"F1 Score:  {f1_score(y_test, y_pred_bert):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk 10.1: Store TF-IDF, CV, and LFS-Voting (P1, P2, P6)\n",
    "\n",
    "# P1: TF-IDF-based best model prediction\n",
    "P1 = y_pred_tfidf\n",
    "\n",
    "# P2: CV-based best model prediction\n",
    "P2 = y_pred_cv\n",
    "\n",
    "# P6: Voting over LFS1+CV, LFS2+CV, LFS3+CV (already done earlier)\n",
    "P6 = y_pred_vote_lfs\n",
    "\n",
    "\n",
    "# Chunk 10.2: Final voting among P1, P2, and P6\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import mode\n",
    "\n",
    "# Stack predictions into a (n_samples, 3) matrix\n",
    "final_preds_matrix = np.vstack((P1, P2, P6)).T\n",
    "\n",
    "# Take mode across axis=1 (row-wise) for hard voting\n",
    "final_prediction = mode(final_preds_matrix, axis=1)[0].flatten()\n",
    "\n",
    "\n",
    "# Chunk 10.3: Evaluate Final WELFake Voting Model\n",
    "print(\"Final WELFake Prediction Performance:\")\n",
    "print(f\"Accuracy:  {accuracy_score(y_test, final_prediction):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, final_prediction):.4f}\")\n",
    "print(f\"Recall:    {recall_score(y_test, final_prediction):.4f}\")\n",
    "print(f\"F1 Score:  {f1_score(y_test, final_prediction):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
