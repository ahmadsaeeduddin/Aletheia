{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.52.4-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting textstat\n",
      "  Downloading textstat-0.7.7-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting textblob\n",
      "  Downloading textblob-0.19.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting filelock (from transformers)\n",
      "  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.30.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.33.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.10/site-packages (from transformers) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.10/site-packages (from transformers) (25.0)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Downloading PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.10/site-packages (from transformers) (2.32.4)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.30.0->transformers)\n",
      "  Downloading fsspec-2025.5.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
      "Collecting hf-xet<2.0.0,>=1.1.2 (from huggingface-hub<1.0,>=0.30.0->transformers)\n",
      "  Downloading hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (879 bytes)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-20.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.10/site-packages (from datasets) (2.3.0)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.30.0->transformers)\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohttp-3.12.13-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.6 kB)\n",
      "Collecting pyphen (from textstat)\n",
      "  Downloading pyphen-0.17.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting cmudict (from textstat)\n",
      "  Downloading cmudict-1.0.32-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.10/site-packages (from textstat) (80.9.0)\n",
      "Requirement already satisfied: nltk>=3.9 in ./.venv/lib/python3.10/site-packages (from textblob) (3.9.1)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading frozenlist-1.7.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading multidict-6.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading propcache-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading yarl-1.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (73 kB)\n",
      "Requirement already satisfied: idna>=2.0 in ./.venv/lib/python3.10/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: click in ./.venv/lib/python3.10/site-packages (from nltk>=3.9->textblob) (8.2.1)\n",
      "Requirement already satisfied: joblib in ./.venv/lib/python3.10/site-packages (from nltk>=3.9->textblob) (1.5.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests->transformers) (2025.6.15)\n",
      "Collecting importlib-metadata>=5 (from cmudict->textstat)\n",
      "  Downloading importlib_metadata-8.7.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting importlib-resources>=5 (from cmudict->textstat)\n",
      "  Downloading importlib_resources-6.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting zipp>=3.20 (from importlib-metadata>=5->cmudict->textstat)\n",
      "  Downloading zipp-3.23.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading transformers-4.52.4-py3-none-any.whl (10.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.33.0-py3-none-any.whl (514 kB)\n",
      "Downloading hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m79.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "Downloading textstat-0.7.7-py3-none-any.whl (175 kB)\n",
      "Downloading textblob-0.19.0-py3-none-any.whl (624 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m624.3/624.3 kB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.12.13-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Downloading multidict-6.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (226 kB)\n",
      "Downloading yarl-1.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (326 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "Downloading frozenlist-1.7.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (222 kB)\n",
      "Downloading propcache-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (198 kB)\n",
      "Downloading pyarrow-20.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (42.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 MB\u001b[0m \u001b[31m77.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (751 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m751.2/751.2 kB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "Downloading cmudict-1.0.32-py3-none-any.whl (939 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m939.4/939.4 kB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading importlib_metadata-8.7.0-py3-none-any.whl (27 kB)\n",
      "Downloading importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Downloading zipp-3.23.0-py3-none-any.whl (10 kB)\n",
      "Downloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Downloading pyphen-0.17.2-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m70.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Installing collected packages: zipp, xxhash, safetensors, pyyaml, pyphen, pyarrow, propcache, multidict, importlib-resources, hf-xet, fsspec, frozenlist, filelock, dill, attrs, async-timeout, aiohappyeyeballs, yarl, textblob, multiprocess, importlib-metadata, huggingface-hub, aiosignal, tokenizers, cmudict, aiohttp, transformers, textstat, datasets\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29/29\u001b[0m [datasets]/29\u001b[0m [datasets]ers]ub]\n",
      "\u001b[1A\u001b[2KSuccessfully installed aiohappyeyeballs-2.6.1 aiohttp-3.12.13 aiosignal-1.3.2 async-timeout-5.0.1 attrs-25.3.0 cmudict-1.0.32 datasets-3.6.0 dill-0.3.8 filelock-3.18.0 frozenlist-1.7.0 fsspec-2025.3.0 hf-xet-1.1.5 huggingface-hub-0.33.0 importlib-metadata-8.7.0 importlib-resources-6.5.2 multidict-6.5.0 multiprocess-0.70.16 propcache-0.3.2 pyarrow-20.0.0 pyphen-0.17.2 pyyaml-6.0.2 safetensors-0.5.3 textblob-0.19.0 textstat-0.7.7 tokenizers-0.21.1 transformers-4.52.4 xxhash-3.5.0 yarl-1.20.1 zipp-3.23.0\n"
     ]
    }
   ],
   "source": [
    "# Chunk 1: Environment Setup and Data Loading\n",
    "\n",
    "# --- 1.1: Install required libraries (uncomment if running first time) ---\n",
    "# !pip install pandas numpy matplotlib seaborn nltk tqdm tensorflow scikit-learn spacy \n",
    "!pip install transformers datasets textstat textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- 1.2: Imports ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataset: (99531, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>Label</th>\n",
       "      <th>text_length</th>\n",
       "      <th>word_count</th>\n",
       "      <th>title_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>donald trump sends out embarrassing new year e...</td>\n",
       "      <td>donald trump just could not wish all american ...</td>\n",
       "      <td>0</td>\n",
       "      <td>2283</td>\n",
       "      <td>385</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>drunk bragging trump staffer started russian c...</td>\n",
       "      <td>house intelligence committee chairman devin nu...</td>\n",
       "      <td>0</td>\n",
       "      <td>1673</td>\n",
       "      <td>248</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sheriff david clarke becomes internet joke for...</td>\n",
       "      <td>friday wa revealed that former milwaukee sheri...</td>\n",
       "      <td>0</td>\n",
       "      <td>2643</td>\n",
       "      <td>422</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>trump obsessed even ha obama name coded into h...</td>\n",
       "      <td>christmas day donald trump announced that woul...</td>\n",
       "      <td>0</td>\n",
       "      <td>2095</td>\n",
       "      <td>338</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pope francis just called out donald trump duri...</td>\n",
       "      <td>pope francis used his annual christmas day mes...</td>\n",
       "      <td>0</td>\n",
       "      <td>1990</td>\n",
       "      <td>332</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  donald trump sends out embarrassing new year e...   \n",
       "1  drunk bragging trump staffer started russian c...   \n",
       "2  sheriff david clarke becomes internet joke for...   \n",
       "3  trump obsessed even ha obama name coded into h...   \n",
       "4  pope francis just called out donald trump duri...   \n",
       "\n",
       "                                                text  Label  text_length  \\\n",
       "0  donald trump just could not wish all american ...      0         2283   \n",
       "1  house intelligence committee chairman devin nu...      0         1673   \n",
       "2  friday wa revealed that former milwaukee sheri...      0         2643   \n",
       "3  christmas day donald trump announced that woul...      0         2095   \n",
       "4  pope francis used his annual christmas day mes...      0         1990   \n",
       "\n",
       "   word_count  title_length  \n",
       "0         385            72  \n",
       "1         248            68  \n",
       "2         422            78  \n",
       "3         338            62  \n",
       "4         332            69  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- 1.3: Load Dataset ---\n",
    "# Replace with your actual path if local file is available\n",
    "\n",
    "df = pd.read_csv('final_data.csv')\n",
    "# --- 1.4: Quick View of Dataset ---\n",
    "print(\"Shape of dataset:\", df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before merging columns:\n",
      "(99531, 6)\n",
      "\n",
      "Shape after merging columns :\n",
      "(99531, 7)\n",
      "\n",
      "Shape after removing null and short values :\n",
      "(99531, 7)\n",
      "\n",
      "Shape after removing duplicates :\n",
      "(61450, 7)\n",
      "Before preprocessing:\n",
      "donald trump sends out embarrassing new year eve message this disturbing donald trump just could not wish all american happy new year and leave that instead had give shout out his enemy hater and the very dishonest fake news medium the former reality show star had just one job and could not our country rapidly grows stronger and smarter want wish all friend supporter enemy hater and even the very dishonest fake news medium happy and healthy new year president angry pant tweeted 2018 will great year for america our country rapidly grows stronger and smarter want wish all friend supporter enemy hater and even the very dishonest fake news medium happy and healthy new year 2018 will great year for america donald trump december 2017trump tweet went down about welll you expect what kind president sends new year greeting like this despicable petty infantile gibberish only trump his lack decency will not even allow him rise above the gutter long enough wish the american citizen happy new year bishop talbert swan december 2017no one like you calvin december 2017your impeachment would make 2018 great year for america but will also accept regaining control congress miranda yaver december 2017do you hear yourself talk when you have include that many people that hate you you have wonder why the they all hate alan sandoval december 2017who us the word hater new year wish marlene december 2017you can not just say happy new year koren pollitt december 2017here trump new year eve tweet from 2016. happy new year all including many enemy and those who have fought and lost badly they just not know what love donald trump december 2016this nothing new for trump been doing this for year trump ha directed message his enemy and hater for new year easter thanksgiving and the anniversary 11. daniel dale december 2017trump holiday tweet are clearly not presidential how long did work hallmark before becoming president steven goodine december 2017he always been like this the only difference that the last few year his filter ha been breaking down roy schulze december 2017who apart from teenager us the term hater wendy december 2017he fucking year old who know december 2017so all the people who voted for this hole thinking would change once got into power you were wrong 70-year-old men not change and now year older photo andrew burton getty image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 61450/61450 [01:19<00:00, 768.73it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After preprocessing:\n",
      "donald trump sends embarrassing new year eve message disturbing donald trump could not wish american happy new year leave instead give shout his enemy hater dishonest fake news medium former reality show star one job could not country rapidly grows stronger smarter want wish friend supporter enemy hater even dishonest fake news medium happy healthy new year president angry pant tweeted will great year america country rapidly grows stronger smarter want wish friend supporter enemy hater even dishonest fake news medium happy healthy new year will great year america donald trump december trump tweet went welll you expect kind president sends new year greeting like despicable petty infantile gibberish trump his lack decency will not even allow rise gutter long enough wish american citizen happy new year bishop talbert swan december no one like you calvin december your impeachment would make great year america will also accept regaining control congress miranda yaver december you hear talk you include many people hate you you wonder they hate alan sandoval december u word hater new year wish marlene december you not say happy new year koren pollitt december trump new year eve tweet happy new year including many enemy fought lost badly they not know love donald trump december nothing new trump year trump ha directed message his enemy hater new year easter thanksgiving anniversary daniel dale december trump holiday tweet are clearly not presidential long work hallmark becoming president steven goodine december he always like difference last year his filter ha breaking roy schulze december apart teenager u term hater wendy december he fucking year old know december people voted hole thinking would change got power you wrong yearold men not change year older photo andrew burton getty image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Chunk 2: Data Preprocessing\n",
    "\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "# --- 2.1: Text Cleaning Function ---\n",
    "def clean_text(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Remove extra whitespaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# --- 2.2: Stopword Removal and Lemmatization ---\n",
    "# Define custom list of important words to preserve\n",
    "important_words = {\n",
    "    'not', 'no', 'never', 'nothing', 'nowhere', 'none', 'nobody',\n",
    "    'would', 'could', 'should', 'will', 'was', 'is', 'are',\n",
    "    'you', 'we', 'he', 'they', 'your', 'his', 'her', 'their'\n",
    "}\n",
    "\n",
    "# Load NLTK stopwords and remove important words\n",
    "stop_words = set(stopwords.words('english')) - important_words\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Clean\n",
    "    text = clean_text(text)\n",
    "    # Tokenize\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # Remove stopwords and lemmatize\n",
    "    processed = [lemmatizer.lemmatize(w) for w in tokens if w not in stop_words]\n",
    "    return ' '.join(processed)\n",
    "\n",
    "# --- 2.3: Apply Preprocessing to the Dataset ---\n",
    "\n",
    "# Display shape before merging columns\n",
    "print(\"Shape before merging columns:\")\n",
    "print(df.shape)\n",
    "\n",
    "# Fill missing text/title values with empty strings (handles NaN)\n",
    "if {'title', 'text'}.issubset(df.columns):\n",
    "    df['content'] = df['title'].fillna('') + ' ' + df['text'].fillna('')\n",
    "else:\n",
    "    df['content'] = df['text'].fillna('')\n",
    "\n",
    "# Display shape after merging columns\n",
    "print(\"\\nShape after merging columns :\")\n",
    "print(df.shape)\n",
    "\n",
    "# Remove null values and values with less than 20 characters\n",
    "df = df.dropna(subset=['content'])\n",
    "df = df[df['content'].str.len() >= 20]\n",
    "# Display shape after removing null and short values\n",
    "print(\"\\nShape after removing null and short values :\")\n",
    "print(df.shape)\n",
    "\n",
    "# Drop duplicate content rows if any\n",
    "df.drop_duplicates(subset='content', inplace=True)\n",
    "\n",
    "# Display shape after removing duplicates\n",
    "print(\"\\nShape after removing duplicates :\")\n",
    "print(df.shape)\n",
    "\n",
    "\n",
    "# Show a sample before preprocessing\n",
    "print(\"Before preprocessing:\")\n",
    "print(df['content'].iloc[0])\n",
    "\n",
    "# Add tqdm progress bar for preprocessing\n",
    "tqdm.pandas()\n",
    "df['processed'] = df['content'].progress_apply(preprocess_text)\n",
    "\n",
    "# Show the result\n",
    "print(\"\\nAfter preprocessing:\")\n",
    "print(df['processed'].iloc[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of Label 0: 30642\n",
      "Count of Label 1: 30808\n"
     ]
    }
   ],
   "source": [
    "# count the number of rows with Label 0 and 1\n",
    "label_0_count = len(df[df['Label'] == 0])\n",
    "label_1_count = len(df[df['Label'] == 1])\n",
    "\n",
    "print(f\"Count of Label 0: {label_0_count}\")\n",
    "print(f\"Count of Label 1: {label_1_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "spacy.cli.download(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 61450/61450 [28:46<00:00, 35.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed correlated features: ['gunning_fog', 'smog', 'ari', 'syllables', 'word_count', 'verbs', 'sentences', 'adjectives', 'adverbs']\n",
      "Remaining features: ['special_chars', 'determiners', 'capital_letters', 'short_sent', 'long_sent', 'polarity', 'title_similarity', 'subjectivity', 'adj_adv_rate', 'words_per_sentence', 'articles']\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['gunning_fog', 'adjectives'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/lab/Documents/i221963/Updated WELFAKE/FIXED.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/lab/Documents/i221963/Updated%20WELFAKE/FIXED.ipynb#W6sZmlsZQ%3D%3D?line=96'>97</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mRemaining features: \u001b[39m\u001b[39m{\u001b[39;00mlf_df_filtered\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mtolist()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/lab/Documents/i221963/Updated%20WELFAKE/FIXED.ipynb#W6sZmlsZQ%3D%3D?line=98'>99</a>\u001b[0m \u001b[39m# --- Manual LFS Grouping (Table V) ---\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/lab/Documents/i221963/Updated%20WELFAKE/FIXED.ipynb#W6sZmlsZQ%3D%3D?line=99'>100</a>\u001b[0m LFS1 \u001b[39m=\u001b[39m lf_df_filtered[[\u001b[39m'\u001b[39;49m\u001b[39mspecial_chars\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mshort_sent\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mlong_sent\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mgunning_fog\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mpolarity\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mtitle_similarity\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39msubjectivity\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39madj_adv_rate\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39marticles\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39madjectives\u001b[39;49m\u001b[39m'\u001b[39;49m]]\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/lab/Documents/i221963/Updated%20WELFAKE/FIXED.ipynb#W6sZmlsZQ%3D%3D?line=100'>101</a>\u001b[0m LFS2 \u001b[39m=\u001b[39m lf_df_filtered[[\u001b[39m'\u001b[39m\u001b[39mdeterminers\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mshort_sent\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mlong_sent\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msmog\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mpolarity\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtitle_similarity\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msubjectivity\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mword_count\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mverbs\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msentences\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/lab/Documents/i221963/Updated%20WELFAKE/FIXED.ipynb#W6sZmlsZQ%3D%3D?line=101'>102</a>\u001b[0m LFS3 \u001b[39m=\u001b[39m lf_df_filtered[[\u001b[39m'\u001b[39m\u001b[39mspecial_chars\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mcapital_letters\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mari\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mpolarity\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtitle_similarity\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msubjectivity\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msyllables\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwords_per_sentence\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39madverbs\u001b[39m\u001b[39m'\u001b[39m]]\n",
      "File \u001b[0;32m~/Documents/i221963/Updated WELFAKE/.venv/lib/python3.10/site-packages/pandas/core/frame.py:4113\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4111\u001b[0m     \u001b[39mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   4112\u001b[0m         key \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(key)\n\u001b[0;32m-> 4113\u001b[0m     indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49m_get_indexer_strict(key, \u001b[39m\"\u001b[39;49m\u001b[39mcolumns\u001b[39;49m\u001b[39m\"\u001b[39;49m)[\u001b[39m1\u001b[39m]\n\u001b[1;32m   4115\u001b[0m \u001b[39m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   4116\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(indexer, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39m==\u001b[39m \u001b[39mbool\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/i221963/Updated WELFAKE/.venv/lib/python3.10/site-packages/pandas/core/indexes/base.py:6212\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6209\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   6210\u001b[0m     keyarr, indexer, new_indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6212\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[1;32m   6214\u001b[0m keyarr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtake(indexer)\n\u001b[1;32m   6215\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6216\u001b[0m     \u001b[39m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/i221963/Updated WELFAKE/.venv/lib/python3.10/site-packages/pandas/core/indexes/base.py:6264\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6261\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNone of [\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m] are in the [\u001b[39m\u001b[39m{\u001b[39;00maxis_name\u001b[39m}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   6263\u001b[0m not_found \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[39m.\u001b[39mnonzero()[\u001b[39m0\u001b[39m]]\u001b[39m.\u001b[39munique())\n\u001b[0;32m-> 6264\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mnot_found\u001b[39m}\u001b[39;00m\u001b[39m not in index\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['gunning_fog', 'adjectives'] not in index\""
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import textstat\n",
    "from tqdm import tqdm\n",
    "from textblob import TextBlob\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "import re\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# --- Full Feature Extraction Function ---\n",
    "def extract_linguistic_features(row):\n",
    "    text = row['processed']\n",
    "    title = row['title'] if 'title' in row else \"\"\n",
    "\n",
    "    doc = nlp(text)\n",
    "    num_words = len([token for token in doc if token.is_alpha])\n",
    "    num_chars = sum(len(token.text) for token in doc if token.is_alpha)\n",
    "    avg_word_length = np.mean([len(token.text) for token in doc if token.is_alpha]) if num_words > 0 else 0\n",
    "    num_sentences = len(list(doc.sents))\n",
    "\n",
    "    num_nouns = len([token for token in doc if token.pos_ == \"NOUN\"])\n",
    "    num_verbs = len([token for token in doc if token.pos_ == \"VERB\"])\n",
    "    num_adj = len([token for token in doc if token.pos_ == \"ADJ\"])\n",
    "    num_adv = len([token for token in doc if token.pos_ == \"ADV\"])\n",
    "    num_dets = len([token for token in doc if token.pos_ == \"DET\"])\n",
    "    num_ents = len(doc.ents)\n",
    "\n",
    "    num_caps = sum(1 for token in doc if token.text.isupper())\n",
    "    num_articles = sum(1 for token in doc if token.text.lower() in {\"a\", \"an\", \"the\"})\n",
    "    num_special_chars = len(re.findall(r'[^a-zA-Z0-9\\s]', text))\n",
    "\n",
    "    sent_lengths = [len([token for token in sent if token.is_alpha]) for sent in doc.sents]\n",
    "    num_short_sent = sum(1 for l in sent_lengths if l <= 5)\n",
    "    num_long_sent = sum(1 for l in sent_lengths if l >= 20)\n",
    "\n",
    "    flesch_read = textstat.flesch_reading_ease(text)\n",
    "    gunning_fog = textstat.gunning_fog(text)\n",
    "    smog = textstat.smog_index(text)\n",
    "    ari = textstat.automated_readability_index(text)\n",
    "\n",
    "    blob = TextBlob(text)\n",
    "    polarity = blob.sentiment.polarity\n",
    "    subjectivity = blob.sentiment.subjectivity\n",
    "    syllables = textstat.syllable_count(text)\n",
    "\n",
    "    if title:\n",
    "        title_vec = nlp(title).vector\n",
    "        text_vec = doc.vector\n",
    "        \n",
    "        # Check if vectors have at least one feature\n",
    "        if len(title_vec) > 0 and len(text_vec) > 0:\n",
    "            sim = cosine_similarity([title_vec], [text_vec])[0][0]\n",
    "        else:\n",
    "            sim = 0.0  # or any other default value\n",
    "    else:\n",
    "        sim = 0.0\n",
    "\n",
    "    adj_adv_rate = (num_adj + num_adv) / num_words if num_words else 0\n",
    "    words_per_sent = num_words / num_sentences if num_sentences else 0\n",
    "\n",
    "    return [\n",
    "        num_special_chars, num_dets, num_caps, num_short_sent, num_long_sent,\n",
    "        gunning_fog, smog, ari,\n",
    "        polarity, sim, subjectivity,\n",
    "        syllables, num_words, adj_adv_rate, words_per_sent,\n",
    "        num_articles, num_verbs, num_sentences,\n",
    "        num_adj, num_adv\n",
    "    ]\n",
    "\n",
    "lf_columns = [\n",
    "    'special_chars', 'determiners', 'capital_letters', 'short_sent', 'long_sent',\n",
    "    'gunning_fog', 'smog', 'ari',\n",
    "    'polarity', 'title_similarity', 'subjectivity',\n",
    "    'syllables', 'word_count', 'adj_adv_rate', 'words_per_sentence',\n",
    "    'articles', 'verbs', 'sentences',\n",
    "    'adjectives', 'adverbs'\n",
    "]\n",
    "\n",
    "# Extract features\n",
    "tqdm.pandas()\n",
    "linguistic_features = df.progress_apply(extract_linguistic_features, axis=1)\n",
    "lf_df = pd.DataFrame(linguistic_features.tolist(), columns=lf_columns)\n",
    "\n",
    "# --- Pearson Correlation Filtering ---\n",
    "cor_matrix = lf_df.corr().abs()\n",
    "upper = cor_matrix.where(np.triu(np.ones(cor_matrix.shape), k=1).astype(bool))\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.7)]\n",
    "lf_df_filtered = lf_df.drop(columns=to_drop)\n",
    "\n",
    "print(f\"Removed correlated features: {to_drop}\")\n",
    "print(f\"Remaining features: {lf_df_filtered.columns.tolist()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample LFS1:\n",
      "   special_chars  short_sent  long_sent  polarity  title_similarity  \\\n",
      "0            0.0   -0.215846  -0.542159  0.570270          0.748534   \n",
      "1            0.0   -0.215846  -0.542159 -0.721587          0.491445   \n",
      "2            0.0    3.360243   0.762522 -0.039679          0.450214   \n",
      "3            0.0   -0.215846  -0.542159 -0.798724         -0.577765   \n",
      "4            0.0   -0.215846   0.762522 -0.381383          0.405477   \n",
      "\n",
      "   subjectivity  adj_adv_rate  articles  \n",
      "0      1.692724      1.390719       0.0  \n",
      "1     -0.592174      0.994764       0.0  \n",
      "2      1.163485     -0.457139       0.0  \n",
      "3      0.128123     -0.489694       0.0  \n",
      "4      0.754988     -1.141160       0.0  \n"
     ]
    }
   ],
   "source": [
    "# --- Manual LFS Grouping (Table V) ---\n",
    "LFS1 = lf_df_filtered[['special_chars', 'short_sent', 'long_sent',  'polarity', 'title_similarity', 'subjectivity', 'adj_adv_rate', 'articles']]\n",
    "LFS2 = lf_df_filtered[['determiners', 'short_sent', 'long_sent',  'polarity', 'title_similarity', 'subjectivity']]\n",
    "LFS3 = lf_df_filtered[['special_chars', 'capital_letters', 'polarity', 'title_similarity', 'subjectivity', 'words_per_sentence']]\n",
    "\n",
    "# --- Normalize ---\n",
    "scaler = StandardScaler()\n",
    "df_lfs1 = pd.DataFrame(scaler.fit_transform(LFS1), columns=LFS1.columns)\n",
    "df_lfs2 = pd.DataFrame(scaler.fit_transform(LFS2), columns=LFS2.columns)\n",
    "df_lfs3 = pd.DataFrame(scaler.fit_transform(LFS3), columns=LFS3.columns)\n",
    "\n",
    "# --- Preview ---\n",
    "print(\"\\nSample LFS1:\")\n",
    "print(df_lfs1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer Accuracy (P2): 0.9171\n",
      "TF-IDF Accuracy (P1): 0.9262\n",
      "Better Word Embedding: TFIDF\n",
      "\n",
      "--- Accuracy Summary ---\n",
      "TF-IDF only (P1): 0.9262\n",
      "CountVectorizer only (P2): 0.9171\n",
      "TFIDF + LFS1: 0.9290\n",
      "TFIDF + LFS2: 0.9289\n",
      "TFIDF + LFS3: 0.9279\n"
     ]
    }
   ],
   "source": [
    "# Chunk 4: Word Embedding + Generate P1 and P2 + Combine with LFS\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.sparse import hstack\n",
    "import pandas as pd\n",
    "\n",
    "# --- 4.1: Prepare train/test split ---\n",
    "X = df['processed']\n",
    "y = df['Label'].astype(int)\n",
    "X_train_text, X_test_text, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- 4.2: Apply CountVectorizer (CV) ---\n",
    "cv_vectorizer = CountVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "X_train_cv = cv_vectorizer.fit_transform(X_train_text)\n",
    "X_test_cv = cv_vectorizer.transform(X_test_text)\n",
    "\n",
    "# --- 4.3: Apply TF-IDF ---\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_text)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test_text)\n",
    "\n",
    "# --- 4.4: Train baseline classifier (Logistic Regression) ---\n",
    "lr_cv = LogisticRegression(max_iter=600)\n",
    "lr_tfidf = LogisticRegression(max_iter=600)\n",
    "\n",
    "lr_cv.fit(X_train_cv, y_train)\n",
    "lr_tfidf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# --- 4.5: Evaluate accuracy ---\n",
    "y_pred_cv = lr_cv.predict(X_test_cv)\n",
    "y_pred_tfidf = lr_tfidf.predict(X_test_tfidf)\n",
    "\n",
    "acc_cv = accuracy_score(y_test, y_pred_cv)\n",
    "acc_tfidf = accuracy_score(y_test, y_pred_tfidf)\n",
    "\n",
    "print(f\"CountVectorizer Accuracy (P2): {acc_cv:.4f}\")\n",
    "print(f\"TF-IDF Accuracy (P1): {acc_tfidf:.4f}\")\n",
    "\n",
    "# --- 4.6: Decide better embedding ---\n",
    "best_embedding = 'cv' if acc_cv > acc_tfidf else 'tfidf'\n",
    "print(f\"Better Word Embedding: {best_embedding.upper()}\")\n",
    "\n",
    "# Save P1 and P2\n",
    "P1 = y_pred_tfidf\n",
    "P2 = y_pred_cv\n",
    "\n",
    "\n",
    "# --- 4.7: Combine CV with LFS1, LFS2, LFS3 ---\n",
    "# Split LFSs into train/test sets\n",
    "train_idx, test_idx = train_test_split(df_lfs1.index, test_size=0.2, random_state=42)\n",
    "df_lfs1_train, df_lfs1_test = df_lfs1.loc[train_idx], df_lfs1.loc[test_idx]\n",
    "df_lfs2_train, df_lfs2_test = df_lfs2.loc[train_idx], df_lfs2.loc[test_idx]\n",
    "df_lfs3_train, df_lfs3_test = df_lfs3.loc[train_idx], df_lfs3.loc[test_idx]\n",
    "# Convert to numpy\n",
    "LFS1_train = df_lfs1_train.values\n",
    "LFS1_test = df_lfs1_test.values\n",
    "LFS2_train = df_lfs2_train.values\n",
    "LFS2_test = df_lfs2_test.values\n",
    "LFS3_train = df_lfs3_train.values\n",
    "LFS3_test = df_lfs3_test.values\n",
    "\n",
    "# Choose the best embedding\n",
    "if best_embedding == 'cv':\n",
    "    X_train_best = X_train_cv\n",
    "    X_test_best = X_test_cv\n",
    "else:\n",
    "    X_train_best = X_train_tfidf\n",
    "    X_test_best = X_test_tfidf\n",
    "\n",
    "# Concatenate best embedding with LFS features\n",
    "X_train_comb1 = hstack([X_train_best, LFS1_train])\n",
    "X_test_comb1 = hstack([X_test_best, LFS1_test])\n",
    "X_train_comb2 = hstack([X_train_best, LFS2_train])\n",
    "X_test_comb2 = hstack([X_test_best, LFS2_test])\n",
    "X_train_comb3 = hstack([X_train_best, LFS3_train])\n",
    "X_test_comb3 = hstack([X_test_best, LFS3_test])\n",
    "\n",
    "# Train classifiers\n",
    "clf1 = LogisticRegression(max_iter=600)\n",
    "clf2 = LogisticRegression(max_iter=600)\n",
    "clf3 = LogisticRegression(max_iter=600)\n",
    "\n",
    "clf1.fit(X_train_comb1, y_train)\n",
    "clf2.fit(X_train_comb2, y_train)\n",
    "clf3.fit(X_train_comb3, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred_comb1 = clf1.predict(X_test_comb1)\n",
    "y_pred_comb2 = clf2.predict(X_test_comb2)\n",
    "y_pred_comb3 = clf3.predict(X_test_comb3)\n",
    "\n",
    "# Evaluate\n",
    "acc_comb1 = accuracy_score(y_test, y_pred_comb1)\n",
    "acc_comb2 = accuracy_score(y_test, y_pred_comb2)\n",
    "acc_comb3 = accuracy_score(y_test, y_pred_comb3)\n",
    "\n",
    "# --- 4.8: Store all accuracies ---\n",
    "accuracy_results = {\n",
    "    \"TF-IDF only (P1)\": acc_tfidf,\n",
    "    \"CountVectorizer only (P2)\": acc_cv,\n",
    "    f\"{best_embedding.upper()} + LFS1\": acc_comb1,\n",
    "    f\"{best_embedding.upper()} + LFS2\": acc_comb2,\n",
    "    f\"{best_embedding.upper()} + LFS3\": acc_comb3\n",
    "}\n",
    "\n",
    "print(\"\\n--- Accuracy Summary ---\")\n",
    "for k, v in accuracy_results.items():\n",
    "    print(f\"{k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
