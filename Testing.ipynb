{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General-purpose\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Torch for modeling\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Scikit-learn for preprocessing and evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# NLP tools\n",
    "import spacy\n",
    "from textblob import TextBlob\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "\n",
    "# Progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Word cloud for visualization\n",
    "from wordcloud import WordCloud\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Good things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>Label</th>\n",
       "      <th>text_length</th>\n",
       "      <th>word_count</th>\n",
       "      <th>title_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>donald trump sends out embarrassing new year e...</td>\n",
       "      <td>donald trump just could not wish all american ...</td>\n",
       "      <td>0</td>\n",
       "      <td>2283</td>\n",
       "      <td>385</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>drunk bragging trump staffer started russian c...</td>\n",
       "      <td>house intelligence committee chairman devin nu...</td>\n",
       "      <td>0</td>\n",
       "      <td>1673</td>\n",
       "      <td>248</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sheriff david clarke becomes internet joke for...</td>\n",
       "      <td>friday wa revealed that former milwaukee sheri...</td>\n",
       "      <td>0</td>\n",
       "      <td>2643</td>\n",
       "      <td>422</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>trump obsessed even ha obama name coded into h...</td>\n",
       "      <td>christmas day donald trump announced that woul...</td>\n",
       "      <td>0</td>\n",
       "      <td>2095</td>\n",
       "      <td>338</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pope francis just called out donald trump duri...</td>\n",
       "      <td>pope francis used his annual christmas day mes...</td>\n",
       "      <td>0</td>\n",
       "      <td>1990</td>\n",
       "      <td>332</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  donald trump sends out embarrassing new year e...   \n",
       "1  drunk bragging trump staffer started russian c...   \n",
       "2  sheriff david clarke becomes internet joke for...   \n",
       "3  trump obsessed even ha obama name coded into h...   \n",
       "4  pope francis just called out donald trump duri...   \n",
       "\n",
       "                                                text  Label  text_length  \\\n",
       "0  donald trump just could not wish all american ...      0         2283   \n",
       "1  house intelligence committee chairman devin nu...      0         1673   \n",
       "2  friday wa revealed that former milwaukee sheri...      0         2643   \n",
       "3  christmas day donald trump announced that woul...      0         2095   \n",
       "4  pope francis used his annual christmas day mes...      0         1990   \n",
       "\n",
       "   word_count  title_length  \n",
       "0         385            72  \n",
       "1         248            68  \n",
       "2         422            78  \n",
       "3         338            62  \n",
       "4         332            69  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dff = pd.read_csv('final_data.csv')\n",
    "dff.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting en-core-web-md==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.8.0/en_core_web_md-3.8.0-py3-none-any.whl (33.5 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 33.5/33.5 MB 63.2 MB/s eta 0:00:00\n",
      "Installing collected packages: en-core-web-md\n",
      "Successfully installed en-core-web-md-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_md')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "spacy.cli.download(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download necessary models\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "# nltk.download('vader_lexicon')\n",
    "sid = SentimentIntensityAnalyzer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device -> cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55d9f5c09f6640c5874033955ef8e4a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/3111 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Device -> {device}')\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n",
    "\n",
    "text_embeddings = model.encode(dff['text'].tolist(), show_progress_bar=True, batch_size=32, convert_to_tensor=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'text_embeddings': torch.tensor(text_embeddings)\n",
    "}, 'embeddings.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just for loading The text_embeddings\n",
    "\n",
    "checkpoint = torch.load('embeddings.pth')\n",
    "\n",
    "# Retrieve embeddings\n",
    "text_embeddings = checkpoint['text_embeddings']\n",
    "\n",
    "text_embeddings = text_embeddings.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Feature Functions on Tokens (per-article) ---\n",
    "\n",
    "def paraphrasing_rate(processed_sentences, T=0.8):\n",
    "    embeddings = [nlp(s['original']).vector for s in processed_sentences]\n",
    "    N = len(embeddings)\n",
    "    if N <= 1:\n",
    "        return 0.0\n",
    "    count = 0\n",
    "    for i in range(N):\n",
    "        sims = [\n",
    "            cosine_similarity([embeddings[i]], [embeddings[j]])[0][0]\n",
    "            for j in range(N) if j != i\n",
    "        ]\n",
    "        if max(sims, default=0) > T:\n",
    "            count += 1\n",
    "    return count / N\n",
    "\n",
    "def subjectivity_ratio(processed_sentences):\n",
    "    N = len(processed_sentences)\n",
    "    if N == 0:\n",
    "        return 0.0\n",
    "    subj = sum(1 for s in processed_sentences\n",
    "               if TextBlob(s['original']).sentiment.subjectivity > 0.5)\n",
    "    return subj / N\n",
    "\n",
    "def sentiment_intensity_ratio(processed_sentences):\n",
    "    N = len(processed_sentences)\n",
    "    if N == 0:\n",
    "        return 0.0\n",
    "    intense = sum(1 for s in processed_sentences\n",
    "                  if abs(sid.polarity_scores(s['original'])['compound']) > 0.5)\n",
    "    return intense / N\n",
    "\n",
    "def average_sentence_length(processed_sentences):\n",
    "    N = len(processed_sentences)\n",
    "    if N == 0:\n",
    "        return 0.0\n",
    "    total = sum(s['word_count'] for s in processed_sentences)\n",
    "    return total / N\n",
    "\n",
    "def manipulative_score(processed_sentences, alpha=0.5, beta=0.5):\n",
    "    return alpha * subjectivity_ratio(processed_sentences) + beta * sentiment_intensity_ratio(processed_sentences)\n",
    "\n",
    "\n",
    "# --- Dataset Preparation ---\n",
    "\n",
    "def prepare_dataset(df, text_vectors):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        rows: list of dicts per article, containing 'processed_sentences' and target 'label'\n",
    "        text_vectors: Nx384 numpy array of text embeddings\n",
    "    Returns:\n",
    "        features: torch.Tensor shape (N, 389), where 384 dims + 5 new features\n",
    "        labels: torch.LongTensor shape (N,)\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for vec, row in zip(text_vectors, df):\n",
    "        f1 = paraphrasing_rate(df['text'])\n",
    "        f2 = subjectivity_ratio(df['text'])\n",
    "        f3 = sentiment_intensity_ratio(df['text'])\n",
    "        f4 = average_sentence_length(df['text'])\n",
    "        f5 = manipulative_score(df['text'])\n",
    "        X.append(list(vec) + [f1, f2, f3, f4, f5])\n",
    "        y.append(df['Label'])\n",
    "    return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "\n",
    "# --- Model Definition ---\n",
    "\n",
    "class FakeNewsClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=389):\n",
    "        super(FakeNewsClassifier, self).__init__()\n",
    "        hidden1 = 512\n",
    "        hidden2 = 256\n",
    "        self.net = nn.Sequential(\n",
    "                    nn.Linear(input_dim, hidden1),\n",
    "                    nn.BatchNorm1d(hidden1),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(0.3),\n",
    "                    nn.Linear(hidden1, hidden2),\n",
    "                    nn.BatchNorm1d(hidden2),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(0.3),\n",
    "                    nn.Linear(hidden2, 1)  # Final output\n",
    "                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.net(x)).squeeze(1)\n",
    "\n",
    "# --- Training and Evaluation Helpers ---\n",
    "\n",
    "def train(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device), yb.to(device).float()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(xb)\n",
    "        loss = criterion(out, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            probs = model(xb)\n",
    "            preds = (probs > 0.5).long()\n",
    "            correct += (preds == yb).sum().item()\n",
    "            total += yb.size(0)\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "# Startin ........................\n",
    "\n",
    "X, y = prepare_dataset(dff, text_embeddings)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=32)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = FakeNewsClassifier().to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "epochs = 5\n",
    "best_acc = 0\n",
    "for epoch in range(epochs):\n",
    "    loss = train(model, train_loader, criterion, optimizer, device)\n",
    "    acc = evaluate(model, test_loader, device)\n",
    "    print(f\"Epoch {epoch+1}/{epochs} — Loss: {loss:.4f}, Test Acc: {acc:.4f}\")\n",
    "\n",
    "    # optional: early stopping or checkpointing\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        torch.save(model.state_dict(), \"best_fake_news.pt\")\n",
    "        print(\"✅ New best!\")\n",
    "\n",
    "print(f\"✅ Training done. Best test accuracy: {best_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
